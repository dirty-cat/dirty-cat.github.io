{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Scalability considerations for similarity encoding\n\nHere we discuss how to apply efficiently SimilarityEncoder to larger\ndatasets: reducing the number of reference categories to \"prototypes\",\neither chosen as the most frequent categories, or with kmeans clustering.\n\nNote that the :class:`GapEncoder` naturally does data reduction and comes\nwith online estimation. As a result is it more scalable than the\nSimilarityEncoder, and should be preferred in large-scale settings.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Avoid the warning in scikit-learn's LogisticRegression for the change\n# in the solver\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A tool to report memory usage and run time\n\nFor this example, we build a small tool that reports memory\nusage and compute time of a function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\nimport functools\nimport tracemalloc\n\n\ndef resource_used(func):\n    \"\"\" Decorator that return a function that prints its usage\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapped_func(*args, **kwargs):\n        t0 = time()\n        tracemalloc.start()\n        out = func(*args, **kwargs)\n        size, peak = tracemalloc.get_traced_memory()\n        tracemalloc.stop()\n        peak /= (1024 ** 2)  # Convert to megabytes\n        print(\"Run time: %.1is    Memory used: %iMb\"\n              % (time() - t0, peak))\n        return out\n\n    return wrapped_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Importing and preprocessing\n\nWe first get the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom dirty_cat.datasets import fetch_open_payments\n\nopen_payments = fetch_open_payments()\nprint(open_payments.description)\n\ndf = open_payments.X\n\nna_mask: pd.DataFrame = df.isna()\ndf = df.dropna(axis=0)\ndf = df.reset_index()\n\nfrom functools import reduce\n\ny = open_payments.y\n# Combine boolean masks\nna_mask = reduce(lambda acc, col: acc | na_mask[col],\n                 na_mask.columns, na_mask[na_mask.columns[0]])\n# Drop the lines that contained missing values in X\ny = y[~na_mask]\ny.reset_index()\n\nclean_columns = [\n    'Applicable_Manufacturer_or_Applicable_GPO_Making_Payment_Name',\n    'Dispute_Status_for_Publication',\n    'Physician_Specialty',\n]\ndirty_columns = [\n    'Name_of_Associated_Covered_Device_or_Medical_Supply1',\n    'Name_of_Associated_Covered_Drug_or_Biological1',\n]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use SimilarityEncoder on the the two dirty columns defined above.\nOne difficulty is that they have many different entries.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df[dirty_columns].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df[dirty_columns].value_counts()[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we will see, SimilarityEncoder takes a while on such data.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SimilarityEncoder with default options\n\nLet us build our vectorizer, using a ColumnTransformer to combine\none-hot encoding and similarity encoding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom dirty_cat import SimilarityEncoder\n\nsim_enc = SimilarityEncoder(similarity='ngram')\n\ntransformers = [\n    ('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'), clean_columns),\n]\n\ncolumn_trans = ColumnTransformer(\n    transformers=transformers + [('sim_enc', sim_enc, dirty_columns)],\n    remainder='drop')\n\nt0 = time()\nX = column_trans.fit_transform(df)\nt1 = time()\nprint('Time to vectorize: %s' % (t1 - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run a cross-validation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model, pipeline, model_selection\n\n# We specify max_iter to avoid convergence warnings\nlog_reg = linear_model.LogisticRegression(max_iter=10000)\n\nmodel = pipeline.make_pipeline(column_trans, log_reg)\nresults = resource_used(model_selection.cross_validate)(model, df, y)\nprint(\"Cross-validation score: %s\" % results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store results for later\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = dict()\nscores['Default options'] = results['test_score']\ntimes = dict()\ntimes['Default options'] = results['fit_time']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Most frequent strategy to define prototypes\n\nThe most frequent strategy selects the n most frequent values in a dirty\ncategorical variable to reduce the dimensionality of the problem and thus\nspeed things up. We select manually the number of prototypes we want to use.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_enc = SimilarityEncoder(similarity='ngram', categories='most_frequent',\n                            n_prototypes=100)\n\ncolumn_trans = ColumnTransformer(\n    transformers=transformers + [('sim_enc', sim_enc, dirty_columns)],\n    remainder='drop')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check now that prediction is still as good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = pipeline.make_pipeline(column_trans, log_reg)\nresults = resource_used(model_selection.cross_validate)(model, df, y)\nprint(\"Cross-validation score: %s\" % results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store results for later\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores['Most frequent'] = results['test_score']\ntimes['Most frequent'] = results['fit_time']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## KMeans strategy to define prototypes\n\nK-means strategy is also a dimensionality reduction technique.\nSimilarityEncoder can apply a K-means and nearest neighbors algorithm\nto find the prototypes. The number of prototypes is set manually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_enc = SimilarityEncoder(similarity='ngram', categories='k-means',\n                            n_prototypes=100)\n\ncolumn_trans = ColumnTransformer(\n    transformers=transformers + [('sim_enc', sim_enc, dirty_columns)],\n    remainder='drop')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check now that prediction is still as good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = pipeline.make_pipeline(column_trans, log_reg)\nresults = resource_used(model_selection.cross_validate)(model, df, y)\nprint(\"Cross-validation score: %s\" % results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store results for later\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores['KMeans'] = results['test_score']\ntimes['KMeans'] = results['fit_time']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot a summary figure\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn\nimport matplotlib.pyplot as plt\n\n_, (ax1, ax2) = plt.subplots(nrows=2, figsize=(4, 3))\nseaborn.boxplot(data=pd.DataFrame(scores), orient='h', ax=ax1)\nax1.set_xlabel('Prediction accuracy', size=16)\n[t.set(size=16) for t in ax1.get_yticklabels()]\n\nseaborn.boxplot(data=pd.DataFrame(times), orient='h', ax=ax2)\nax2.set_xlabel('Computation time', size=16)\n[t.set(size=16) for t in ax2.get_yticklabels()]\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}