{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Comparing encoders of a dirty categorical columns\n\nThe column *Employee Position Title* of the dataset `employee salaries\n<https://catalog.data.gov/dataset/employee-salaries-2016>`_ contains dirty categorical\ndata.\n\nHere, we compare different categorical encodings for the dirty column to\npredict the *Current Annual Salary*, using gradient boosted trees.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Data Importing and preprocessing\n\nWe first download the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_employee_salaries\nemployee_salaries = fetch_employee_salaries()\nprint(employee_salaries['DESCR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\ndf = employee_salaries['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's carry out some basic preprocessing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df['Date First Hired'] = pd.to_datetime(df['date_first_hired'])\ndf['Year First Hired'] = df['Date First Hired'].apply(lambda x: x.year)\n# drop rows with NaN in gender\ndf.dropna(subset=['gender'], inplace=True)\n\ntarget_column = 'Current Annual Salary'\ny = df[target_column].values.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Choosing columns\nFor categorical columns that are supposed to be clean, it is \"safe\" to\nuse one hot encoding to transform them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clean_columns = {\n    'gender': 'one-hot',\n    'department_name': 'one-hot',\n    'assignment_category': 'one-hot',\n    'Year First Hired': 'numerical'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then choose the categorical encoding methods we want to benchmark\nand the dirty categorical variable:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoding_methods = ['one-hot', 'target', 'similarity', 'minhash',\n                    'gap']\ndirty_column = 'employee_position_title'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Creating a learning pipeline\nThe encoders for both clean and dirty data are first imported:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dirty_cat import SimilarityEncoder, TargetEncoder, MinHashEncoder,\\\n    GapEncoder\n\n# for scikit-learn 0.24 we need to require the experimental feature\nfrom sklearn.experimental import enable_hist_gradient_boosting  # noqa\n# now you can import normally from ensemble\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nencoders_dict = {\n    'one-hot': OneHotEncoder(handle_unknown='ignore', sparse=False),\n    'similarity': SimilarityEncoder(similarity='ngram'),\n    'target': TargetEncoder(handle_unknown='ignore'),\n    'minhash': MinHashEncoder(n_components=100),\n    'gap': GapEncoder(n_components=100),\n    'numerical': FunctionTransformer(None)}\n\n# We then create a function that takes one key of our ``encoders_dict``,\n# returns a pipeline object with the associated encoder,\n# as well as a gradient-boosting regressor\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\ndef assemble_pipeline(encoding_method):\n    # static transformers from the other columns\n    transformers = [(enc + '_' + col, encoders_dict[enc], [col])\n                    for col, enc in clean_columns.items()]\n    # adding the encoded column\n    transformers += [(encoding_method, encoders_dict[encoding_method],\n                      [dirty_column])]\n    pipeline = Pipeline([\n        # Use ColumnTransformer to combine the features\n        ('union', ColumnTransformer(\n            transformers=transformers,\n            remainder='drop')),\n        ('clf', HistGradientBoostingRegressor())\n    ])\n    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using each encoding for supervised learning\nEventually, we loop over the different encoding methods,\ninstantiate each time a new pipeline, fit it\nand store the returned cross-validation score:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\nimport numpy as np\n\nall_scores = dict()\n\nfor method in encoding_methods:\n    pipeline = assemble_pipeline(method)\n    scores = cross_val_score(pipeline, df, y)\n    print('{} encoding'.format(method))\n    print('r2 score:  mean: {:.3f}; std: {:.3f}\\n'.format(\n        np.mean(scores), np.std(scores)))\n    all_scores[method] = scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plotting the results\nFinally, we plot the scores on a boxplot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(4, 3))\nax = seaborn.boxplot(data=pd.DataFrame(all_scores), orient='h')\nplt.ylabel('Encoding', size=20)\nplt.xlabel('Prediction accuracy     ', size=20)\nplt.yticks(size=20)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The clear trend is that encoders that use the string form\nof the category (similarity, minhash, and gap) perform better than\nthose that discard it.\n\nSimilarityEncoder is the best performer, but it is less scalable on big\ndata than MinHashEncoder and GapEncoder. The most scalable encoder is\nthe MinHashEncoder. GapEncoder, on the other hand, has the benefit that\nit provides interpretable features (see `sphx_glr_auto_examples_04_feature_interpretation_gap_encoder.py`)\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}