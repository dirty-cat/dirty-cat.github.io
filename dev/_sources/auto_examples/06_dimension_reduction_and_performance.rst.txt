
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/06_dimension_reduction_and_performance.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_06_dimension_reduction_and_performance.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_06_dimension_reduction_and_performance.py:


Scalability considerations for similarity encoding
===================================================

Here we discuss how to apply efficiently SimilarityEncoder to larger
datasets: reducing the number of reference categories to "prototypes",
either chosen as the most frequent categories, or with kmeans clustering.

Note that the :class:`GapEncoder` naturally does data reduction and comes
with online estimation. As a result is it more scalable than the
SimilarityEncoder, and should be prefered in large-scale settings.

.. GENERATED FROM PYTHON SOURCE LINES 14-19

.. code-block:: default

    # Avoid the warning in scikit-learn's LogisticRegression for the change
    # in the solver
    import warnings
    warnings.simplefilter(action='ignore', category=FutureWarning)








.. GENERATED FROM PYTHON SOURCE LINES 20-25

A tool to report memory usage and run time
-------------------------------------------

For this example, we build a small tool that reports memory
usage and compute time of a function

.. GENERATED FROM PYTHON SOURCE LINES 25-49

.. code-block:: default

    from time import time
    import functools
    import tracemalloc


    def resource_used(func):
        """ Decorator that return a function that prints its usage
        """

        @functools.wraps(func)
        def wrapped_func(*args, **kwargs):
            t0 = time()
            tracemalloc.start()
            out = func(*args, **kwargs)
            size, peak = tracemalloc.get_traced_memory()
            tracemalloc.stop()
            peak /= (1024 ** 2)  # Convert to megabytes
            print("Run time: %.1is    Memory used: %iMb"
                  % (time() - t0, peak))
            return out

        return wrapped_func









.. GENERATED FROM PYTHON SOURCE LINES 50-54

Data Importing and preprocessing
--------------------------------

We first download the dataset:

.. GENERATED FROM PYTHON SOURCE LINES 54-59

.. code-block:: default

    from dirty_cat.datasets import fetch_traffic_violations

    data = fetch_traffic_violations()
    print(data['description'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/circleci/project/dirty_cat/datasets/fetching.py:207: UserWarning: content size cannot be found, downloading file from https://data.montgomerycountymd.gov/api/views/4mse-ku6q/rows.csv?accessType=DOWNLOAD as a whole
      warnings.warn('content size cannot be found, '
    The downloaded data contains the traffic_violations dataset.
    It can originally be found at: https://catalog.data.gov/dataset/ traffic-violations-56dda




.. GENERATED FROM PYTHON SOURCE LINES 60-61

Then we load it:

.. GENERATED FROM PYTHON SOURCE LINES 61-67

.. code-block:: default

    import pandas as pd

    # Limit to 50 000 rows, for a faster example
    df = pd.read_csv(data['path'], nrows=50000)
    df = df.dropna(axis=0)
    df = df.reset_index()




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    /home/circleci/miniconda/envs/testenv/lib/python3.8/site-packages/sphinx_gallery/gen_rst.py:516: DtypeWarning: Columns (20,22,24,25) have mixed types.Specify dtype option on import or set low_memory=False.
      exec(self.code, self.fake_main.__dict__)




.. GENERATED FROM PYTHON SOURCE LINES 68-70

We will use SimilarityEncoder on the 'description' column. One
difficulty is that it has many different entries.

.. GENERATED FROM PYTHON SOURCE LINES 70-72

.. code-block:: default

    print(df['Description'].nunique())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    263




.. GENERATED FROM PYTHON SOURCE LINES 73-75

.. code-block:: default

    print(df['Description'].value_counts()[:20])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    DRIVING VEH. WHILE IMPAIRED BY ALCOHOL                                                               171
    DRIVING VEHICLE WHILE UNDER THE INFLUENCE OF ALCOHOL                                                 169
    NEGLIGENT DRIVING VEHICLE IN CARELESS AND IMPRUDENT MANNER ENDANGERING PROPERTY, LIFE AND PERSON     139
    RECKLESS DRIVING VEHICLE IN WANTON AND WILLFUL DISREGARD FOR SAFETY OF PERSONS AND PROPERTY          106
    DRIVING VEHICLE WHILE UNDER THE INFLUENCE OF ALCOHOL PER SE                                          103
    DRIVER FAILURE TO OBEY PROPERLY PLACED TRAFFIC CONTROL DEVICE INSTRUCTIONS                            80
    PERSON DRIVING MOTOR VEHICLE ON HIGHWAY OR PUBLIC USE PROPERTY ON SUSPENDED LICENSE AND PRIVILEGE     48
    FAILURE OF INDIVIDUAL DRIVING ON HIGHWAY TO DISPLAY LICENSE TO UNIFORMED POLICE ON DEMAND             48
    FAILURE TO DRIVE VEHICLE ON RIGHT HALF OF ROADWAY WHEN REQUIRED                                       42
    DRIVER CHANGING LANES WHEN UNSAFE                                                                     41
    DRIVING MOTOR VEHICLE ON HIGHWAY WITHOUT REQUIRED LICENSE AND AUTHORIZATION                           40
    FAILURE TO CONTROL VEHICLE SPEED ON HIGHWAY TO AVOID COLLISION                                        36
    FAILURE TO DISPLAY REGISTRATION CARD UPON DEMAND BY POLICE OFFICER                                    36
    DRIVING WHILE LIC. SUSP. UNDER 16-203, 16-206A2 FAIL TO ATTEND DIP, 17-106, 26-204/206, 27-103        28
    DRIVING VEH. WHILE IMPAIRED BY A CONTROLLED DANGEROUS SUBSTANCE                                       23
    DRIVING VEHICLE IN EXCESS OF REASONABLE AND PRUDENT SPEED ON HIGHWAY                                  21
    DRIVER FAILURE TO STOP AT STOP SIGN LINE                                                              20
    OPERATING UNREGISTERED MOTOR VEHICLE ON HIGHWAY                                                       15
    PERSON DRIVING MOTOR VEHICLE ON HIGHWAY OR PUBLIC USE PROPERTY ON REVOKED LICENSE AND PRIVILEGE       15
    DRIVING TO DRIVE MOTOR VEHICLE ON HIGHWAY WITHOUT REQUIRED LICENSE AND AUTHORIZATION                  15
    Name: Description, dtype: int64




.. GENERATED FROM PYTHON SOURCE LINES 76-77

As we will see, SimilarityEncoder takes a while on such data.

.. GENERATED FROM PYTHON SOURCE LINES 80-85

SimilarityEncoder with default options
--------------------------------------

Let us build our vectorizer, using a ColumnTransformer to combine
one-hot encoding and similarity encoding

.. GENERATED FROM PYTHON SOURCE LINES 85-117

.. code-block:: default

    from sklearn.preprocessing import OneHotEncoder
    from sklearn.compose import ColumnTransformer
    from dirty_cat import SimilarityEncoder

    sim_enc = SimilarityEncoder(similarity='ngram')

    y = df['Violation Type']

    # clean columns
    transformers = [('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'),
                     ['Alcohol',
                      'Arrest Type',
                      'Belts',
                      'Commercial License',
                      'Commercial Vehicle',
                      'Fatal',
                      'HAZMAT',
                      'Property Damage',
                      'Work Zone']),
                    ('pass', 'passthrough', ['Year']),
                    ]

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')

    t0 = time()
    X = column_trans.fit_transform(df)
    t1 = time()
    print('Time to vectorize: %s' % (t1 - t0))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Time to vectorize: 0.22316718101501465




.. GENERATED FROM PYTHON SOURCE LINES 118-119

We can run a cross-validation

.. GENERATED FROM PYTHON SOURCE LINES 119-128

.. code-block:: default

    from sklearn import linear_model, pipeline, model_selection

    # We specify max_iter to avoid convergence warnings
    log_reg = linear_model.LogisticRegression(max_iter=10000)

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y, )
    print("Cross-validation score: %s" % results['test_score'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 2s    Memory used: 9Mb
    Cross-validation score: [0.94397759 0.94397759 0.94101124 0.94662921 0.94101124]




.. GENERATED FROM PYTHON SOURCE LINES 129-130

Store results for later

.. GENERATED FROM PYTHON SOURCE LINES 130-135

.. code-block:: default

    scores = dict()
    scores['Default options'] = results['test_score']
    times = dict()
    times['Default options'] = results['fit_time']








.. GENERATED FROM PYTHON SOURCE LINES 136-142

Most frequent strategy to define prototypes
---------------------------------------------

The most frequent strategy selects the n most frequent values in a dirty
categorical variable to reduce the dimensionality of the problem and thus
speed things up. We select manually the number of prototypes we want to use.

.. GENERATED FROM PYTHON SOURCE LINES 142-150

.. code-block:: default

    sim_enc = SimilarityEncoder(similarity='ngram', categories='most_frequent',
                                n_prototypes=100)

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')








.. GENERATED FROM PYTHON SOURCE LINES 151-152

Check now that prediction is still as good

.. GENERATED FROM PYTHON SOURCE LINES 152-156

.. code-block:: default

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y)
    print("Cross-validation score: %s" % results['test_score'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 1s    Memory used: 5Mb
    Cross-validation score: [0.94397759 0.94117647 0.94101124 0.94101124 0.94101124]




.. GENERATED FROM PYTHON SOURCE LINES 157-158

Store results for later

.. GENERATED FROM PYTHON SOURCE LINES 158-161

.. code-block:: default

    scores['Most frequent'] = results['test_score']
    times['Most frequent'] = results['fit_time']








.. GENERATED FROM PYTHON SOURCE LINES 162-168

KMeans strategy to define prototypes
---------------------------------------

K-means strategy is also a dimensionality reduction technique.
SimilarityEncoder can apply a K-means and nearest neighbors algorithm
to find the prototypes. The number of prototypes is set manually.

.. GENERATED FROM PYTHON SOURCE LINES 168-176

.. code-block:: default

    sim_enc = SimilarityEncoder(similarity='ngram', categories='k-means',
                                n_prototypes=100)

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')








.. GENERATED FROM PYTHON SOURCE LINES 177-178

Check now that prediction is still as good

.. GENERATED FROM PYTHON SOURCE LINES 178-182

.. code-block:: default

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y)
    print("Cross-validation score: %s" % results['test_score'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 4s    Memory used: 5Mb
    Cross-validation score: [0.93837535 0.93837535 0.93539326 0.94382022 0.93539326]




.. GENERATED FROM PYTHON SOURCE LINES 183-184

Store results for later

.. GENERATED FROM PYTHON SOURCE LINES 184-187

.. code-block:: default

    scores['KMeans'] = results['test_score']
    times['KMeans'] = results['fit_time']








.. GENERATED FROM PYTHON SOURCE LINES 188-190

Plot a summary figure
----------------------

.. GENERATED FROM PYTHON SOURCE LINES 190-203

.. code-block:: default

    import seaborn
    import matplotlib.pyplot as plt

    _, (ax1, ax2) = plt.subplots(nrows=2, figsize=(4, 3))
    seaborn.boxplot(data=pd.DataFrame(scores), orient='h', ax=ax1)
    ax1.set_xlabel('Prediction accuracy', size=16)
    [t.set(size=16) for t in ax1.get_yticklabels()]

    seaborn.boxplot(data=pd.DataFrame(times), orient='h', ax=ax2)
    ax2.set_xlabel('Computation time', size=16)
    [t.set(size=16) for t in ax2.get_yticklabels()]
    plt.tight_layout()




.. image:: /auto_examples/images/sphx_glr_06_dimension_reduction_and_performance_001.png
    :alt: 06 dimension reduction and performance
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 204-210

Reduce memory usage during encoding using float32
----------------------------------------------------------------

We use a float32 dtype in this example to show some speed and memory gains.
The use of the scikit-learn model may upcast to float64 (depending on the used
algorithm). The memory savings will then happen during the encoding.

.. GENERATED FROM PYTHON SOURCE LINES 210-243

.. code-block:: default

    import numpy as np

    sim_enc = SimilarityEncoder(similarity='ngram', dtype=np.float32,
                                categories='most_frequent', n_prototypes=100)

    y = df['Violation Type']
    # cast the year column to float32
    df['Year'] = df['Year'].astype(np.float32)
    # clean columns
    transformers = [('one_hot', OneHotEncoder(sparse=False, dtype=np.float32,
                                              handle_unknown='ignore'),
                     ['Alcohol',
                      'Arrest Type',
                      'Belts',
                      'Commercial License',
                      'Commercial Vehicle',
                      'Fatal',
                      'HAZMAT',
                      'Property Damage',
                      'Work Zone']),
                    ('pass', 'passthrough', ['Year']),
                    ]

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')

    t0 = time()
    X = column_trans.fit_transform(df)
    t1 = time()
    print('Time to vectorize: %s' % (t1 - t0))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Time to vectorize: 0.15165209770202637




.. GENERATED FROM PYTHON SOURCE LINES 244-245

We can run a cross-validation to confirm the memory footprint reduction

.. GENERATED FROM PYTHON SOURCE LINES 245-248

.. code-block:: default

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y, )
    print("Cross-validation score: %s" % results['test_score'])




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 1s    Memory used: 3Mb
    Cross-validation score: [0.93837535 0.94117647 0.94101124 0.94101124 0.94101124]





.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 3 minutes  20.493 seconds)


.. _sphx_glr_download_auto_examples_06_dimension_reduction_and_performance.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/dirty-cat/dirty-cat.github.io/master?filepath=dev/auto_examples/06_dimension_reduction_and_performance.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 06_dimension_reduction_and_performance.py <06_dimension_reduction_and_performance.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 06_dimension_reduction_and_performance.ipynb <06_dimension_reduction_and_performance.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
