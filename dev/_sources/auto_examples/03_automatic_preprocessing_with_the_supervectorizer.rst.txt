
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/03_automatic_preprocessing_with_the_supervectorizer.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_03_automatic_preprocessing_with_the_supervectorizer.py>`
        to download the full example code or to run this example in your browser via Binder

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_03_automatic_preprocessing_with_the_supervectorizer.py:


Automatic pre-processing with the SuperVectorizer
=================================================

In this notebook, we introduce the |SV|, which automatically
turns a heterogeneous dataset into a numerical representation, finding
the right transformers to apply to the different columns.

We demonstrate it on the `employee salaries` dataset.

.. |SV| replace::
    :class:`~dirty_cat.SuperVectorizer`

.. |OneHotEncoder| replace::
    :class:`~sklearn.preprocessing.OneHotEncoder`

.. |ColumnTransformer| replace::
    :class:`~sklearn.compose.ColumnTransformer`

.. |RandomForestRegressor| replace::
    :class:`~sklearn.ensemble.RandomForestRegressor`

.. |SE| replace:: :class:`~dirty_cat.SimilarityEncoder`

.. |permutation importances| replace::
    :func:`~sklearn.inspection.permutation_importance`

.. GENERATED FROM PYTHON SOURCE LINES 31-35

Importing the data
------------------
Let's fetch the dataset, and load X (the employees' features) and y
(the salary to predict):

.. GENERATED FROM PYTHON SOURCE LINES 35-40

.. code-block:: default

    from dirty_cat.gap_encoder import GapEncoder
    from dirty_cat.datasets import fetch_employee_salaries
    employee_salaries = fetch_employee_salaries()
    print(employee_salaries['DESCR'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Annual salary information including gross pay and overtime pay for all active, permanent employees of Montgomery County, MD paid in calendar year 2016. This information will be published annually each year.

    Downloaded from openml.org.




.. GENERATED FROM PYTHON SOURCE LINES 41-57

.. code-block:: default


    X = employee_salaries['data']
    y = employee_salaries['target']
    # We'll drop a few columns we don't want
    X.drop(
        [
            'Current Annual Salary',  # Too linked with target
            'full_name',  # Not relevant to the analysis
            '2016_gross_pay_received',  # Too linked with target
            '2016_overtime_pay',  # Too linked with target
            'date_first_hired'  # Redundant with "year_first_hired"
        ],
        axis=1,
        inplace=True
    )








.. GENERATED FROM PYTHON SOURCE LINES 58-59

The data are in a fairly complex and heterogeneous dataframe:

.. GENERATED FROM PYTHON SOURCE LINES 59-61

.. code-block:: default

    X






.. raw:: html

    <div class="output_subarea output_html rendered_html output_result">
    <div>
    <style scoped>
        .dataframe tbody tr th:only-of-type {
            vertical-align: middle;
        }

        .dataframe tbody tr th {
            vertical-align: top;
        }

        .dataframe thead th {
            text-align: right;
        }
    </style>
    <table border="1" class="dataframe">
      <thead>
        <tr style="text-align: right;">
          <th></th>
          <th>gender</th>
          <th>department</th>
          <th>department_name</th>
          <th>division</th>
          <th>assignment_category</th>
          <th>employee_position_title</th>
          <th>underfilled_job_title</th>
          <th>year_first_hired</th>
        </tr>
      </thead>
      <tbody>
        <tr>
          <th>0</th>
          <td>F</td>
          <td>POL</td>
          <td>Department of Police</td>
          <td>MSB Information Mgmt and Tech Division Records...</td>
          <td>Fulltime-Regular</td>
          <td>Office Services Coordinator</td>
          <td>None</td>
          <td>1986.0</td>
        </tr>
        <tr>
          <th>1</th>
          <td>M</td>
          <td>POL</td>
          <td>Department of Police</td>
          <td>ISB Major Crimes Division Fugitive Section</td>
          <td>Fulltime-Regular</td>
          <td>Master Police Officer</td>
          <td>None</td>
          <td>1988.0</td>
        </tr>
        <tr>
          <th>2</th>
          <td>F</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>Adult Protective and Case Management Services</td>
          <td>Fulltime-Regular</td>
          <td>Social Worker IV</td>
          <td>None</td>
          <td>1989.0</td>
        </tr>
        <tr>
          <th>3</th>
          <td>M</td>
          <td>COR</td>
          <td>Correction and Rehabilitation</td>
          <td>PRRS Facility and Security</td>
          <td>Fulltime-Regular</td>
          <td>Resident Supervisor II</td>
          <td>None</td>
          <td>2014.0</td>
        </tr>
        <tr>
          <th>4</th>
          <td>M</td>
          <td>HCA</td>
          <td>Department of Housing and Community Affairs</td>
          <td>Affordable Housing Programs</td>
          <td>Fulltime-Regular</td>
          <td>Planning Specialist III</td>
          <td>None</td>
          <td>2007.0</td>
        </tr>
        <tr>
          <th>...</th>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
          <td>...</td>
        </tr>
        <tr>
          <th>9223</th>
          <td>F</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>School Based Health Centers</td>
          <td>Fulltime-Regular</td>
          <td>Community Health Nurse II</td>
          <td>None</td>
          <td>2015.0</td>
        </tr>
        <tr>
          <th>9224</th>
          <td>F</td>
          <td>FRS</td>
          <td>Fire and Rescue Services</td>
          <td>Human Resources Division</td>
          <td>Fulltime-Regular</td>
          <td>Fire/Rescue Division Chief</td>
          <td>None</td>
          <td>1988.0</td>
        </tr>
        <tr>
          <th>9225</th>
          <td>M</td>
          <td>HHS</td>
          <td>Department of Health and Human Services</td>
          <td>Child and Adolescent Mental Health Clinic Serv...</td>
          <td>Parttime-Regular</td>
          <td>Medical Doctor IV - Psychiatrist</td>
          <td>None</td>
          <td>2001.0</td>
        </tr>
        <tr>
          <th>9226</th>
          <td>M</td>
          <td>CCL</td>
          <td>County Council</td>
          <td>Council Central Staff</td>
          <td>Fulltime-Regular</td>
          <td>Manager II</td>
          <td>None</td>
          <td>2006.0</td>
        </tr>
        <tr>
          <th>9227</th>
          <td>M</td>
          <td>DLC</td>
          <td>Department of Liquor Control</td>
          <td>Licensure, Regulation and Education</td>
          <td>Fulltime-Regular</td>
          <td>Alcohol/Tobacco Enforcement Specialist II</td>
          <td>None</td>
          <td>2012.0</td>
        </tr>
      </tbody>
    </table>
    <p>9228 rows Ã— 8 columns</p>
    </div>
    </div>
    <br />
    <br />

.. GENERATED FROM PYTHON SOURCE LINES 62-64

The challenge is to turn this dataframe into a form suited for
machine learning.

.. GENERATED FROM PYTHON SOURCE LINES 66-74

Using the SuperVectorizer in a supervised-learning pipeline
------------------------------------------------------------

Assembling the |SV| in a pipeline with a powerful learner,
such as gradient boosted trees, gives **a machine-learning method that
can be readily applied to the dataframe**.

It's the typical and recommended way of using it.

.. GENERATED FROM PYTHON SOURCE LINES 74-89

.. code-block:: default



    # For scikit-learn 0.24, we need to require the experimental feature
    from sklearn.experimental import enable_hist_gradient_boosting
    from sklearn.ensemble import HistGradientBoostingRegressor

    from sklearn.pipeline import Pipeline

    from dirty_cat import SuperVectorizer

    pipeline = Pipeline([
        ('vectorizer', SuperVectorizer(auto_cast=True)),
        ('clf', HistGradientBoostingRegressor(random_state=42))
    ])








.. GENERATED FROM PYTHON SOURCE LINES 90-91

Let's perform a cross-validation to see how well this model predicts

.. GENERATED FROM PYTHON SOURCE LINES 91-101

.. code-block:: default


    from sklearn.model_selection import cross_val_score

    scores = cross_val_score(pipeline, X, y, scoring='r2')

    import numpy as np
    print(f'{scores=}')
    print(f'mean={np.mean(scores)}')
    print(f'std={np.std(scores)}')





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    scores=array([0.90436579, 0.85940967, 0.88976568, 0.88293046, 0.91330898])
    mean=0.889956116376877
    std=0.018630494144431086




.. GENERATED FROM PYTHON SOURCE LINES 102-106

The prediction perform here is pretty much as good as in :ref:`example
02<sphx_glr_auto_examples_02_fit_predict_plot_employee_salaries.py>`,
but the code here is much simpler as it does not involve specifying
columns manually.

.. GENERATED FROM PYTHON SOURCE LINES 108-113

Analyzing the features created
-------------------------------

Let us perform the same workflow, but without the `Pipeline`, so we can
analyze its mechanisms along the way.

.. GENERATED FROM PYTHON SOURCE LINES 113-119

.. code-block:: default

    sup_vec = SuperVectorizer(
        auto_cast=True,
        high_card_str_transformer=GapEncoder(n_components=50),
        high_card_cat_transformer=GapEncoder(n_components=50)
    )








.. GENERATED FROM PYTHON SOURCE LINES 120-121

We split the data between train and test, and transform them:

.. GENERATED FROM PYTHON SOURCE LINES 121-129

.. code-block:: default

    from sklearn.model_selection import train_test_split
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.15, random_state=42
    )

    X_train_enc = sup_vec.fit_transform(X_train, y_train)
    X_test_enc = sup_vec.transform(X_test)








.. GENERATED FROM PYTHON SOURCE LINES 130-134

Inspecting the features created
.................................
Once it has been trained on data,
we can print the transformers and the columns assignment it creates:

.. GENERATED FROM PYTHON SOURCE LINES 134-137

.. code-block:: default


    sup_vec.transformers_





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    [('high_card_str', GapEncoder(n_components=50), ['division', 'employee_position_title', 'underfilled_job_title']), ('low_card_cat', OneHotEncoder(), ['gender', 'assignment_category']), ('high_card_cat', GapEncoder(n_components=50), ['department', 'department_name']), ('remainder', 'passthrough', [7])]



.. GENERATED FROM PYTHON SOURCE LINES 138-151

This is what is being passed to the |ColumnTransformer| under the hood.
If you're familiar with how the later works, it should be very intuitive.
We can notice it classified the columns "gender" and "assignment_category"
as low cardinality string variables.
A |OneHotEncoder| will be applied to these columns.

The vectorizer actually makes the difference between string variables
(data type ``object`` and ``string``) and categorical variables
(data type ``category``).

Next, we can have a look at the encoded feature names.

Before encoding:

.. GENERATED FROM PYTHON SOURCE LINES 151-153

.. code-block:: default

    X.columns.to_list()





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ['gender', 'department', 'department_name', 'division', 'assignment_category', 'employee_position_title', 'underfilled_job_title', 'year_first_hired']



.. GENERATED FROM PYTHON SOURCE LINES 154-155

After encoding (we only plot the first 8 feature names):

.. GENERATED FROM PYTHON SOURCE LINES 155-158

.. code-block:: default

    feature_names = sup_vec.get_feature_names()
    feature_names[:8]





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    ['division: compliance, employee, explosive', 'division: school, health, head', 'division: station, state, estate', 'division: engineering, parking, marking', 'division: patrol, 3rd, 2nd', 'division: security, mccf, minority', 'division: silver, spring, ride', 'division: safety, traffic, life']



.. GENERATED FROM PYTHON SOURCE LINES 159-165

As we can see, it created a new column for each unique value.
This is because we used |SE| on the column "division",
which was classified as a high cardinality string variable.
(default values, see |SV|'s docstring).

In total, we have 256 encoded columns.

.. GENERATED FROM PYTHON SOURCE LINES 165-168

.. code-block:: default

    len(feature_names)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    256



.. GENERATED FROM PYTHON SOURCE LINES 169-179

Feature importance in the statistical model
............................................
In this section, we will train a regressor, and plot the feature importances
.. topic:: Note:

   To minimize compute time, use the feature importances computed by the
   |RandomForestRegressor|, but you should prefer |permutation importances|
   instead (which are less subject to biases)

First, let's train the |RandomForestRegressor|,

.. GENERATED FROM PYTHON SOURCE LINES 179-185

.. code-block:: default


    from sklearn.ensemble import RandomForestRegressor
    regressor = RandomForestRegressor()
    regressor.fit(X_train_enc, y_train)






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    RandomForestRegressor()



.. GENERATED FROM PYTHON SOURCE LINES 186-187

Getting the feature importances

.. GENERATED FROM PYTHON SOURCE LINES 187-197

.. code-block:: default

    importances = regressor.feature_importances_
    std = np.std(
        [
            tree.feature_importances_
            for tree in regressor.estimators_
        ],
        axis=0
    )
    indices = np.argsort(importances)[::-1]








.. GENERATED FROM PYTHON SOURCE LINES 198-199

Plotting the results:

.. GENERATED FROM PYTHON SOURCE LINES 199-211

.. code-block:: default


    import matplotlib.pyplot as plt
    plt.figure(figsize=(12, 9))
    plt.title("Feature importances")
    n = 20
    n_indices = indices[:n]
    labels = np.array(feature_names)[n_indices]
    plt.barh(range(n), importances[n_indices], color="b", yerr=std[n_indices])
    plt.yticks(range(n), labels, size=15)
    plt.tight_layout(pad=1)
    plt.show()




.. image:: /auto_examples/images/sphx_glr_03_automatic_preprocessing_with_the_supervectorizer_001.png
    :alt: Feature importances
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 212-215

We can deduce from this data that the three factors that define the
most the salary are: being hired for a long time, being a manager, and
having a permanent, full-time job :).


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  50.893 seconds)


.. _sphx_glr_download_auto_examples_03_automatic_preprocessing_with_the_supervectorizer.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example


  .. container:: binder-badge

    .. image:: images/binder_badge_logo.svg
      :target: https://mybinder.org/v2/gh/dirty-cat/dirty-cat.github.io/master?filepath=dev/auto_examples/03_automatic_preprocessing_with_the_supervectorizer.ipynb
      :alt: Launch binder
      :width: 150 px


  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 03_automatic_preprocessing_with_the_supervectorizer.py <03_automatic_preprocessing_with_the_supervectorizer.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 03_automatic_preprocessing_with_the_supervectorizer.ipynb <03_automatic_preprocessing_with_the_supervectorizer.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
