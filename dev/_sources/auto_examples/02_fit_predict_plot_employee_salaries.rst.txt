
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/02_fit_predict_plot_employee_salaries.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_02_fit_predict_plot_employee_salaries.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_02_fit_predict_plot_employee_salaries.py:


Comparing encoders of a dirty categorical columns
==================================================

The column *Employee Position Title* of the dataset `employee salaries
<https://catalog.data.gov/dataset/employee-salaries-2016>`_ contains dirty categorical
data.

Here, we compare different categorical encodings for the dirty column to
predict the *Current Annual Salary*, using gradient boosted trees.

.. GENERATED FROM PYTHON SOURCE LINES 14-18

Data Importing and preprocessing
--------------------------------

We first download the dataset:

.. GENERATED FROM PYTHON SOURCE LINES 18-23

.. code-block:: default

    from dirty_cat.datasets import fetch_employee_salaries
    employee_salaries = fetch_employee_salaries()
    print(employee_salaries['DESCR'])






.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Annual salary information including gross pay and overtime pay for all active, permanent employees of Montgomery County, MD paid in calendar year 2016. This information will be published annually each year.

    Downloaded from openml.org.




.. GENERATED FROM PYTHON SOURCE LINES 24-25

Then we load it:

.. GENERATED FROM PYTHON SOURCE LINES 25-28

.. code-block:: default

    import pandas as pd
    df = employee_salaries['data']








.. GENERATED FROM PYTHON SOURCE LINES 29-30

Now, let's carry out some basic preprocessing:

.. GENERATED FROM PYTHON SOURCE LINES 30-38

.. code-block:: default

    df['Date First Hired'] = pd.to_datetime(df['date_first_hired'])
    df['Year First Hired'] = df['Date First Hired'].apply(lambda x: x.year)
    # drop rows with NaN in gender
    df.dropna(subset=['gender'], inplace=True)

    target_column = 'Current Annual Salary'
    y = df[target_column].values.ravel()








.. GENERATED FROM PYTHON SOURCE LINES 39-43

Choosing columns
-----------------
For categorical columns that are supposed to be clean, it is "safe" to
use one hot encoding to transform them:

.. GENERATED FROM PYTHON SOURCE LINES 43-50

.. code-block:: default


    clean_columns = {
        'gender': 'one-hot',
        'department_name': 'one-hot',
        'assignment_category': 'one-hot',
        'Year First Hired': 'numerical'}








.. GENERATED FROM PYTHON SOURCE LINES 51-53

We then choose the categorical encoding methods we want to benchmark
and the dirty categorical variable:

.. GENERATED FROM PYTHON SOURCE LINES 53-57

.. code-block:: default


    encoding_methods = ['one-hot', 'target', 'similarity', 'minhash',
                        'gap']
    dirty_column = 'employee_position_title'







.. GENERATED FROM PYTHON SOURCE LINES 61-64

Creating a learning pipeline
----------------------------
The encoders for both clean and dirty data are first imported:

.. GENERATED FROM PYTHON SOURCE LINES 64-108

.. code-block:: default


    from sklearn.preprocessing import FunctionTransformer
    from sklearn.preprocessing import OneHotEncoder
    from dirty_cat import SimilarityEncoder, TargetEncoder, MinHashEncoder,\
        GapEncoder

    # for scikit-learn 0.24 we need to require the experimental feature
    from sklearn.experimental import enable_hist_gradient_boosting  # noqa
    # now you can import normally from ensemble
    from sklearn.ensemble import HistGradientBoostingRegressor

    encoders_dict = {
        'one-hot': OneHotEncoder(handle_unknown='ignore', sparse=False),
        'similarity': SimilarityEncoder(similarity='ngram'),
        'target': TargetEncoder(handle_unknown='ignore'),
        'minhash': MinHashEncoder(n_components=100),
        'gap': GapEncoder(n_components=100),
        'numerical': FunctionTransformer(None)}

    # We then create a function that takes one key of our ``encoders_dict``,
    # returns a pipeline object with the associated encoder,
    # as well as a gradient-boosting regressor

    from sklearn.compose import ColumnTransformer
    from sklearn.pipeline import Pipeline


    def assemble_pipeline(encoding_method):
        # static transformers from the other columns
        transformers = [(enc + '_' + col, encoders_dict[enc], [col])
                        for col, enc in clean_columns.items()]
        # adding the encoded column
        transformers += [(encoding_method, encoders_dict[encoding_method],
                          [dirty_column])]
        pipeline = Pipeline([
            # Use ColumnTransformer to combine the features
            ('union', ColumnTransformer(
                transformers=transformers,
                remainder='drop')),
            ('clf', HistGradientBoostingRegressor())
        ])
        return pipeline









.. GENERATED FROM PYTHON SOURCE LINES 109-114

Using each encoding for supervised learning
--------------------------------------------
Eventually, we loop over the different encoding methods,
instantiate each time a new pipeline, fit it
and store the returned cross-validation score:

.. GENERATED FROM PYTHON SOURCE LINES 114-128

.. code-block:: default


    from sklearn.model_selection import cross_val_score
    import numpy as np

    all_scores = dict()

    for method in encoding_methods:
        pipeline = assemble_pipeline(method)
        scores = cross_val_score(pipeline, df, y)
        print('{} encoding'.format(method))
        print('r2 score:  mean: {:.3f}; std: {:.3f}\n'.format(
            np.mean(scores), np.std(scores)))
        all_scores[method] = scores





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    one-hot encoding
    r2 score:  mean: 0.776; std: 0.028

    target encoding
    r2 score:  mean: 0.842; std: 0.030

    similarity encoding
    r2 score:  mean: 0.923; std: 0.014

    minhash encoding
    r2 score:  mean: 0.919; std: 0.012

    gap encoding
    r2 score:  mean: 0.908; std: 0.016





.. GENERATED FROM PYTHON SOURCE LINES 129-132

Plotting the results
--------------------
Finally, we plot the scores on a boxplot:

.. GENERATED FROM PYTHON SOURCE LINES 132-142

.. code-block:: default


    import seaborn
    import matplotlib.pyplot as plt
    plt.figure(figsize=(4, 3))
    ax = seaborn.boxplot(data=pd.DataFrame(all_scores), orient='h')
    plt.ylabel('Encoding', size=20)
    plt.xlabel('Prediction accuracy     ', size=20)
    plt.yticks(size=20)
    plt.tight_layout()




.. image:: /auto_examples/images/sphx_glr_02_fit_predict_plot_employee_salaries_001.png
    :alt: 02 fit predict plot employee salaries
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 143-151

The clear trend is that encoders that use the string form
of the category (similarity, minhash, and gap) perform better than
those that discard it.

SimilarityEncoder is the best performer, but it is less scalable on big
data than MinHashEncoder and GapEncoder. The most scalable encoder is
the MinHashEncoder. GapEncoder, on the other hand, has the benefit that
it provides interpretable features (see :ref:`sphx_glr_auto_examples_04_feature_interpretation_gap_encoder.py`)


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 1 minutes  46.466 seconds)


.. _sphx_glr_download_auto_examples_02_fit_predict_plot_employee_salaries.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 02_fit_predict_plot_employee_salaries.py <02_fit_predict_plot_employee_salaries.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 02_fit_predict_plot_employee_salaries.ipynb <02_fit_predict_plot_employee_salaries.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
