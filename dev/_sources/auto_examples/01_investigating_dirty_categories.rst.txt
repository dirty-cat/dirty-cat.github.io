
.. DO NOT EDIT.
.. THIS FILE WAS AUTOMATICALLY GENERATED BY SPHINX-GALLERY.
.. TO MAKE CHANGES, EDIT THE SOURCE PYTHON FILE:
.. "auto_examples/01_investigating_dirty_categories.py"
.. LINE NUMBERS ARE GIVEN BELOW.

.. only:: html

    .. note::
        :class: sphx-glr-download-link-note

        Click :ref:`here <sphx_glr_download_auto_examples_01_investigating_dirty_categories.py>`
        to download the full example code

.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_01_investigating_dirty_categories.py:


Investigating dirty categories
=================================

What are dirty categorical variables and how can a good encoding help
with statistical learning.

.. GENERATED FROM PYTHON SOURCE LINES 10-14

What do we mean by dirty categories?
-------------------------------------------------

Let's look at a dataset called employee salaries:

.. GENERATED FROM PYTHON SOURCE LINES 14-22

.. code-block:: default

    import pandas as pd
    from dirty_cat import datasets

    employee_salaries = datasets.fetch_employee_salaries()
    print(employee_salaries['DESCR'])
    data = employee_salaries['data']
    print(data.head(n=5))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Annual salary information including gross pay and overtime pay for all active, permanent employees of Montgomery County, MD paid in calendar year 2016. This information will be published annually each year.

    Downloaded from openml.org.
                full_name gender  ...  year_first_hired  Current Annual Salary
    0      Aarhus, Pam J.      F  ...            1986.0               69222.18
    1     Aaron, David J.      M  ...            1988.0               97392.47
    2    Aaron, Marsha M.      F  ...            1989.0              104717.28
    3  Ababio, Godfred A.      M  ...            2014.0               52734.57
    4      Ababu, Essayas      M  ...            2007.0               93396.00

    [5 rows x 13 columns]




.. GENERATED FROM PYTHON SOURCE LINES 23-24

Here is how many unique entries there is per column

.. GENERATED FROM PYTHON SOURCE LINES 24-26

.. code-block:: default

    print(data.nunique())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    full_name                  9222
    gender                        2
    2016_gross_pay_received    8977
    2016_overtime_pay          6176
    department                   37
    department_name              37
    division                    694
    assignment_category           2
    employee_position_title     385
    underfilled_job_title        84
    date_first_hired           2264
    year_first_hired             51
    Current Annual Salary      3403
    dtype: int64




.. GENERATED FROM PYTHON SOURCE LINES 27-28

As we can see, some entries have many different unique values:

.. GENERATED FROM PYTHON SOURCE LINES 28-30

.. code-block:: default

    print(data['employee_position_title'].value_counts().sort_index())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Abandoned Vehicle Code Enforcement Specialist        4
    Accountant/Auditor I                                 3
    Accountant/Auditor II                                1
    Accountant/Auditor III                              35
    Administrative Assistant to the County Executive     1
                                                        ..
    Welder                                               3
    Work Force Leader I                                  1
    Work Force Leader II                                28
    Work Force Leader III                                2
    Work Force Leader IV                                 9
    Name: employee_position_title, Length: 385, dtype: int64




.. GENERATED FROM PYTHON SOURCE LINES 31-50

These different entries are often variations on the same entities:
there are 3 kinds of Accountant/Auditor.

Such variations will break traditional categorical encoding methods:

* Using simple one-hot encoding will create orthogonal features,
  whereas it is clear that those 3 terms have a lot in common.

* If we wanted to use word embedding methods such as word2vec,
  we would have to go through a cleaning phase: those algorithms
  are not trained to work on data such as 'Accountant/Auditor I'.
  However, this can be error prone and time consuming.

The problem becomes easier if we can capture relationships between
entries.

To simplify understanding, we will focus on the column describing the
employee's position title:
data

.. GENERATED FROM PYTHON SOURCE LINES 50-52

.. code-block:: default

    values = data[['employee_position_title', 'gender', 'Current Annual Salary']]








.. GENERATED FROM PYTHON SOURCE LINES 53-59

String similarity between entries
-------------------------------------------------

That's where our encoders get into play. In order to robustly
embed dirty semantic data, the SimilarityEncoder creates a similarity
matrix based on the 3-gram structure of the data.

.. GENERATED FROM PYTHON SOURCE LINES 59-67

.. code-block:: default

    sorted_values = values['employee_position_title'].sort_values().unique()

    from dirty_cat import SimilarityEncoder

    similarity_encoder = SimilarityEncoder(similarity='ngram')
    transformed_values = similarity_encoder.fit_transform(
        sorted_values.reshape(-1, 1))








.. GENERATED FROM PYTHON SOURCE LINES 68-73

Plotting the new representation using multi-dimensional scaling
................................................................

Let's now plot a couple points at random using a low-dimensional representation
to get an intuition of what the similarity encoder is doing:

.. GENERATED FROM PYTHON SOURCE LINES 73-82

.. code-block:: default

    from sklearn.manifold import MDS

    mds = MDS(dissimilarity='precomputed', n_init=10, random_state=42)
    two_dim_data = mds.fit_transform(
        1 - transformed_values)  # transformed values lie
    # in the 0-1 range, so 1-transformed_value yields a positive dissimilarity matrix
    print(two_dim_data.shape)
    print(sorted_values.shape)





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    (385, 2)
    (385,)




.. GENERATED FROM PYTHON SOURCE LINES 83-84

We first quickly fit a KNN so that the plots does not get too busy:

.. GENERATED FROM PYTHON SOURCE LINES 84-96

.. code-block:: default

    import numpy as np

    n_points = 5
    np.random.seed(42)
    from sklearn.neighbors import NearestNeighbors

    random_points = np.random.choice(len(similarity_encoder.categories_[0]),
                                     n_points, replace=False)
    nn = NearestNeighbors(n_neighbors=2).fit(transformed_values)
    _, indices_ = nn.kneighbors(transformed_values[random_points])
    indices = np.unique(indices_.squeeze())








.. GENERATED FROM PYTHON SOURCE LINES 97-98

Then we plot it, adding the categories in the scatter plot:

.. GENERATED FROM PYTHON SOURCE LINES 98-110

.. code-block:: default


    import matplotlib.pyplot as plt

    f, ax = plt.subplots()
    ax.scatter(x=two_dim_data[indices, 0], y=two_dim_data[indices, 1])
    # adding the legend
    for x in indices:
        ax.text(x=two_dim_data[x, 0], y=two_dim_data[x, 1], s=sorted_values[x],
                fontsize=8)
    ax.set_title(
        'multi-dimensional-scaling representation using a 3gram similarity matrix')




.. image:: /auto_examples/images/sphx_glr_01_investigating_dirty_categories_001.png
    :alt: multi-dimensional-scaling representation using a 3gram similarity matrix
    :class: sphx-glr-single-img


.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none


    Text(0.5, 1.0, 'multi-dimensional-scaling representation using a 3gram similarity matrix')



.. GENERATED FROM PYTHON SOURCE LINES 111-115

Heatmap of the similarity matrix
................................

We can also plot the distance matrix for those observations:

.. GENERATED FROM PYTHON SOURCE LINES 115-126

.. code-block:: default

    f2, ax2 = plt.subplots(figsize=(6, 6))
    cax2 = ax2.matshow(transformed_values[indices, :][:, indices])
    ax2.set_yticks(np.arange(len(indices)))
    ax2.set_xticks(np.arange(len(indices)))
    ax2.set_yticklabels(sorted_values[indices], rotation='30')
    ax2.set_xticklabels(sorted_values[indices], rotation='60', ha='right')
    ax2.xaxis.tick_bottom()
    ax2.set_title('Similarities across categories')
    f2.colorbar(cax2)
    f2.tight_layout()




.. image:: /auto_examples/images/sphx_glr_01_investigating_dirty_categories_002.png
    :alt: Similarities across categories
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 127-133

As shown in the previous plot, we see that the nearest neighbor of
"Communication Equipment Technician"
is "telecommunication technician", although it is also
very close to senior "supply technician": therefore, we grasp the
"communication" part (not initially present in the category as a unique word)
as well as the technician part of this category.

.. GENERATED FROM PYTHON SOURCE LINES 136-141

Encoding categorical data using SimilarityEncoder
-------------------------------------------------

A typical data-science workflow uses one-hot encoding to represent
categories.

.. GENERATED FROM PYTHON SOURCE LINES 141-155

.. code-block:: default

    from sklearn.preprocessing import OneHotEncoder

    # encoding simply a subset of the observations
    n_obs = 20
    employee_position_titles = values['employee_position_title'].head(
        n_obs).to_frame()
    categorical_encoder = OneHotEncoder(sparse=False)
    one_hot_encoded = categorical_encoder.fit_transform(employee_position_titles)
    f3, ax3 = plt.subplots(figsize=(6, 6))
    ax3.matshow(one_hot_encoded)
    ax3.set_title('Employee Position Title values, one-hot encoded')
    ax3.axis('off')
    f3.tight_layout()




.. image:: /auto_examples/images/sphx_glr_01_investigating_dirty_categories_003.png
    :alt: Employee Position Title values, one-hot encoded
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 156-160

The corresponding is very sparse

SimilarityEncoder can be used to replace one-hot encoding capturing the
similarities:

.. GENERATED FROM PYTHON SOURCE LINES 160-168

.. code-block:: default


    f4, ax4 = plt.subplots(figsize=(6, 6))
    similarity_encoded = similarity_encoder.fit_transform(employee_position_titles)
    ax4.matshow(similarity_encoded)
    ax4.set_title('Employee Position Title values, similarity encoded')
    ax4.axis('off')
    f4.tight_layout()




.. image:: /auto_examples/images/sphx_glr_01_investigating_dirty_categories_004.png
    :alt: Employee Position Title values, similarity encoded
    :class: sphx-glr-single-img





.. GENERATED FROM PYTHON SOURCE LINES 169-171

Other examples in the dirty_cat documentation show how
similarity encoding impacts prediction performance.


.. rst-class:: sphx-glr-timing

   **Total running time of the script:** ( 2 minutes  44.346 seconds)


.. _sphx_glr_download_auto_examples_01_investigating_dirty_categories.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download sphx-glr-download-python

     :download:`Download Python source code: 01_investigating_dirty_categories.py <01_investigating_dirty_categories.py>`



  .. container:: sphx-glr-download sphx-glr-download-jupyter

     :download:`Download Jupyter notebook: 01_investigating_dirty_categories.ipynb <01_investigating_dirty_categories.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.github.io>`_
