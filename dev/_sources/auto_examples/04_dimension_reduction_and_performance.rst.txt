.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_04_dimension_reduction_and_performance.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_04_dimension_reduction_and_performance.py:


Scalability considerations for similarity encoding
===================================================

Here we discuss how to apply efficiently SimilarityEncoder to larger
datasets: reducing the number of reference categories to "prototypes",
either chosen as the most frequent categories, or with kmeans clustering.




.. code-block:: python

    # Avoid the warning in scikit-learn's LogisticRegression for the change
    # in the solver
    import warnings
    warnings.simplefilter(action='ignore', category=FutureWarning)







A tool to report memory usage and run time
-------------------------------------------

For this example, we build a small tool that reports memory
usage and compute time of a function



.. code-block:: python

    from time import time
    import functools
    import memory_profiler


    def resource_used(func):
        """ Decorator that return a function that prints its usage
        """

        @functools.wraps(func)
        def wrapped_func(*args, **kwargs):
            t0 = time()
            mem, out = memory_profiler.memory_usage((func, args, kwargs),
                                                    max_usage=True,
                                                    retval=True)
            print("Run time: %.1is    Memory used: %iMb"
                  % (time() - t0, mem[0]))
            return out

        return wrapped_func








Data Importing and preprocessing
--------------------------------

We first download the dataset:



.. code-block:: python

    from dirty_cat.datasets import fetch_traffic_violations

    data = fetch_traffic_violations()
    print(data['description'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    The downloaded data contains the traffic_violations dataset.
    It can originally be found at: https://catalog.data.gov/dataset/ traffic-violations-56dda


Then we load it:



.. code-block:: python

    import pandas as pd

    df = pd.read_csv(data['path'])

    # Limit to 50 000 rows, for a faster example
    df = df[:50000].copy()
    df = df.dropna(axis=0)
    df = df.reset_index()






We will use SimilarityEncoder on the 'description' column. One
difficulty is that it many different entries



.. code-block:: python

    print(df['Description'].nunique())





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    1760



.. code-block:: python

    print(df['Description'].value_counts()[:20])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    DRIVER FAILURE TO OBEY PROPERLY PLACED TRAFFIC CONTROL DEVICE INSTRUCTIONS                              2958
    FAILURE TO DISPLAY REGISTRATION CARD UPON DEMAND BY POLICE OFFICER                                      1827
    DRIVING VEHICLE ON HIGHWAY WITH SUSPENDED REGISTRATION                                                  1813
    PERSON DRIVING MOTOR VEHICLE ON HIGHWAY OR PUBLIC USE PROPERTY ON SUSPENDED LICENSE AND PRIVILEGE       1495
    FAILURE OF INDIVIDUAL DRIVING ON HIGHWAY TO DISPLAY LICENSE TO UNIFORMED POLICE ON DEMAND               1476
    DISPLAYING EXPIRED REGISTRATION PLATE ISSUED BY ANY STATE                                               1248
    DRIVING WHILE IMPAIRED BY ALCOHOL                                                                        826
    DRIVING VEHICLE WHILE UNDER THE INFLUENCE OF ALCOHOL                                                     799
    DRIVER USING HANDS TO USE HANDHELD TELEPHONE WHILEMOTOR VEHICLE IS IN MOTION                             786
    DRIVING VEHICLE ON HIGHWAY WITHOUT CURRENT REGISTRATION PLATES AND VALIDATION TABS                       782
    DRIVING MOTOR VEHICLE ON HIGHWAY WITHOUT REQUIRED LICENSE AND AUTHORIZATION                              762
    DRIVER FAILURE TO STOP AT STOP SIGN LINE                                                                 748
    FAILURE TO CONTROL VEHICLE SPEED ON HIGHWAY TO AVOID COLLISION                                           739
    DRIVER CHANGING LANES WHEN UNSAFE                                                                        709
    NEGLIGENT DRIVING VEHICLE IN CARELESS AND IMPRUDENT MANNER ENDANGERING PROPERTY, LIFE AND PERSON         689
    DRIVER FAILURE TO STOP AT STEADY CIRCULAR RED SIGNAL                                                     618
    OPERATOR NOT RESTRAINED BY SEATBELT                                                                      593
    DRIVING TO DRIVE MOTOR VEHICLE ON HIGHWAY WITHOUT REQUIRED LICENSE AND AUTHORIZATION                     555
    DRIVING UNDER THE INFLUENCE OF ALCOHOL PER SE                                                            539
    FAILURE OF VEH. ON HWY. TO DISPLAY LIGHTED LAMPS, ILLUMINATING DEVICE IN UNFAVORABLE VISIBILITY COND     533
    Name: Description, dtype: int64


As we will see,SimilarityEncoder takes a while on such data


SimilarityEncoder with default options
--------------------------------------

Let us build our vectorizer, using a ColumnTransformer to combine
one-hot encoding and similarity encoding



.. code-block:: python

    from sklearn.preprocessing import OneHotEncoder
    from sklearn.compose import ColumnTransformer
    from dirty_cat import SimilarityEncoder

    sim_enc = SimilarityEncoder(similarity='ngram')

    y = df['Violation Type']

    # clean columns
    transformers = [('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'),
                     ['Alcohol',
                      'Arrest Type',
                      'Belts',
                      'Commercial License',
                      'Commercial Vehicle',
                      'Fatal',
                      'Gender',
                      'HAZMAT',
                      'Property Damage',
                      'Race',
                      'Work Zone']),
                    ('pass', 'passthrough', ['Year']),
                    ]

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')

    t0 = time()
    X = column_trans.fit_transform(df)
    t1 = time()
    print('Time to vectorize: %s' % (t1 - t0))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Time to vectorize: 8.127359390258789


We can run a cross-validation



.. code-block:: python

    from sklearn import linear_model, pipeline, model_selection

    log_reg = linear_model.LogisticRegression()

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y, )
    print("Cross-validation score: %s" % results['test_score'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 85s    Memory used: 1846Mb
    Cross-validation score: [0.7898622  0.78381609 0.79090206]


Store results for later



.. code-block:: python

    scores = dict()
    scores['Default options'] = results['test_score']
    times = dict()
    times['Default options'] = results['fit_time']







Most frequent strategy to define prototypes
---------------------------------------------

The most frequent strategy selects the n most frequent values in a dirty
categorical variable to reduce the dimensionality of the problem and thus
speed things up. We select manually the number of prototypes we want to use.



.. code-block:: python

    sim_enc = SimilarityEncoder(similarity='ngram', categories='most_frequent',
                                n_prototypes=100)

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')







Check now that prediction is still as good



.. code-block:: python

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y)
    print("Cross-validation score: %s" % results['test_score'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 15s    Memory used: 1260Mb
    Cross-validation score: [0.78620641 0.78522216 0.78492582]


Store results for later



.. code-block:: python

    scores['Most frequent'] = results['test_score']
    times['Most frequent'] = results['fit_time']







KMeans strategy to define prototypes
---------------------------------------

K-means strategy is also a dimensionality reduction technique.
SimilarityEncoder can apply a K-means and nearest neighbors algorithm
to find the prototypes. The number of prototypes is set manually.



.. code-block:: python

    sim_enc = SimilarityEncoder(similarity='ngram', categories='k-means',
                                n_prototypes=100)

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')







Check now that prediction is still as good



.. code-block:: python

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y)
    print("Cross-validation score: %s" % results['test_score'])





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 22s    Memory used: 1259Mb
    Cross-validation score: [0.7821288  0.78473003 0.78612107]


Store results for later



.. code-block:: python

    scores['KMeans'] = results['test_score']
    times['KMeans'] = results['fit_time']







Plot a summary figure
----------------------



.. code-block:: python

    import seaborn
    import matplotlib.pyplot as plt

    _, (ax1, ax2) = plt.subplots(nrows=2, figsize=(4, 3))
    seaborn.boxplot(data=pd.DataFrame(scores), orient='h', ax=ax1)
    ax1.set_xlabel('Prediction accuracy', size=16)
    [t.set(size=16) for t in ax1.get_yticklabels()]

    seaborn.boxplot(data=pd.DataFrame(times), orient='h', ax=ax2)
    ax2.set_xlabel('Computation time', size=16)
    [t.set(size=16) for t in ax2.get_yticklabels()]
    plt.tight_layout()




.. image:: /auto_examples/images/sphx_glr_04_dimension_reduction_and_performance_001.png
    :class: sphx-glr-single-img




Reduce memory usage during encoding using float32
----------------------------------------------------------------

We use a float32 dtype in this example to show some speed and memory gains.
The use of the scikit-learn model may upcast to float64 (depending on the used
algorithm). The memory savings will then happen during the encoding.



.. code-block:: python

    import numpy as np

    sim_enc = SimilarityEncoder(similarity='ngram', dtype=np.float32,
                                categories='most_frequent', n_prototypes=100)

    y = df['Violation Type']
    # cast the year column to float32
    df['Year'] = df['Year'].astype(np.float32)
    # clean columns
    transformers = [('one_hot', OneHotEncoder(sparse=False, dtype=np.float32,
                                              handle_unknown='ignore'),
                     ['Alcohol',
                      'Arrest Type',
                      'Belts',
                      'Commercial License',
                      'Commercial Vehicle',
                      'Fatal',
                      'Gender',
                      'HAZMAT',
                      'Property Damage',
                      'Race',
                      'Work Zone']),
                    ('pass', 'passthrough', ['Year']),
                    ]

    column_trans = ColumnTransformer(
        # adding the dirty column
        transformers=transformers + [('sim_enc', sim_enc, ['Description'])],
        remainder='drop')

    t0 = time()
    X = column_trans.fit_transform(df)
    t1 = time()
    print('Time to vectorize: %s' % (t1 - t0))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Time to vectorize: 1.993370532989502


We can run a cross-validation to confirm the memory footprint reduction



.. code-block:: python

    model = pipeline.make_pipeline(column_trans, log_reg)
    results = resource_used(model_selection.cross_validate)(model, df, y, )
    print("Cross-validation score: %s" % results['test_score'])




.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    Run time: 16s    Memory used: 671Mb
    Cross-validation score: [0.78620641 0.78522216 0.78492582]


**Total running time of the script:** ( 4 minutes  40.069 seconds)


.. _sphx_glr_download_auto_examples_04_dimension_reduction_and_performance.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: 04_dimension_reduction_and_performance.py <04_dimension_reduction_and_performance.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: 04_dimension_reduction_and_performance.ipynb <04_dimension_reduction_and_performance.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
