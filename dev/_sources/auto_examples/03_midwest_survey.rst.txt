.. note::
    :class: sphx-glr-download-link-note

    Click :ref:`here <sphx_glr_download_auto_examples_03_midwest_survey.py>` to download the full example code
.. rst-class:: sphx-glr-example-title

.. _sphx_glr_auto_examples_03_midwest_survey.py:


Semantic variation in the "Midwest"
===================================
Here's some survey data with one dirty column, consisting of
an open-ended question, on which one-hot encoding does not work well.
The other columns are more traditional categorical or numerical
variables.

Let's see how different encoding for the dirty column impact on the
score of a classification problem.



Loading the data
----------------



.. code-block:: python

    from dirty_cat.datasets import fetch_midwest_survey
    import pandas as pd

    dataset = fetch_midwest_survey()
    df = pd.read_csv(dataset['path']).astype(str)







Separating clean, and dirty columns as well a a column we will try to predict
------------------------------------------------------------------------------



.. code-block:: python

    target_column = 'Location (Census Region)'
    dirty_column = 'In your own words, what would you call the part of the country you live in now?'
    clean_columns = [
        'Personally identification as a Midwesterner?',
        'Illinois in MW?',
        'Indiana in MW?',
        'Kansas in MW?',
        'Iowa in MW?',
        'Michigan in MW?',
        'Minnesota in MW?',
        'Missouri in MW?',
        'Nebraska in MW?',
        'North Dakota in MW?',
        'Ohio in MW?',
        'South Dakota in MW?',
        'Wisconsin in MW?',
        'Arkansas in MW?',
        'Colorado in MW?',
        'Kentucky in MW?',
        'Oklahoma in MW?',
        'Pennsylvania in MW?',
        'West Virginia in MW?',
        'Montana in MW?',
        'Wyoming in MW?',
        'Gender',
        'Age',
        'Household Income',
        'Education']
    y = df[target_column].values.ravel()







A pipeline for data fitting and prediction
-------------------------------------------
 we first import the right encoders to transform our clean/dirty data:



.. code-block:: python

    from sklearn.preprocessing import FunctionTransformer, CategoricalEncoder
    from dirty_cat import SimilarityEncoder

    encoder_dict = {
        'one-hot': CategoricalEncoder(handle_unknown='ignore',
                                      encoding='onehot-dense'),
        'similarity': SimilarityEncoder(similarity='ngram',
                                        handle_unknown='ignore'),
        'num': FunctionTransformer(None)
    }






All the clean columns are encoded once and for all, but since we
benchmark different categorical encodings for the dirty variable,
we create a function that takes an encoding as an input, and returns a \
scikit-learn pipeline for our problem.



.. code-block:: python

    from sklearn.pipeline import Pipeline
    from sklearn.compose import ColumnTransformer
    from sklearn.preprocessing import StandardScaler
    from sklearn.ensemble import RandomForestClassifier


    def make_pipeline(encoding_method):
        # static transformers from the other columns
        transformers = [('one-hot-clean', encoder_dict['one-hot'], clean_columns)]
        # adding the encoded column
        transformers += [(encoding_method + '-dirty', encoder_dict[encoding_method],
                          [dirty_column])]
        pipeline = Pipeline([
            # Use ColumnTransformer to combine the features
            ('union', ColumnTransformer(
                transformers=transformers,
                remainder='drop')),
            ('scaler', StandardScaler(with_mean=False)),
            ('classifier', RandomForestClassifier(random_state=5))
        ])

        return pipeline








Evaluation of different encoding methods
-----------------------------------------
We then loop over encoding methods, scoring the different pipeline predictions
using a cross validation score:



.. code-block:: python

    from sklearn.model_selection import cross_val_score
    from sklearn.model_selection import StratifiedKFold

    cv = StratifiedKFold(n_splits=3, random_state=12, shuffle=True)
    all_scores = {}
    for method in ['one-hot', 'similarity']:
        pipeline = make_pipeline(method)
        # Now predict the census region of each participant
        scores = cross_val_score(pipeline, df, y, cv=cv)
        all_scores[method] = scores

        print('%s encoding' % method)
        print('Accuracy score:  mean: %.3f; std: %.3f\n'
              % (scores.mean(), scores.std()))





.. rst-class:: sphx-glr-script-out

 Out:

 .. code-block:: none

    one-hot encoding
    Accuracy score:  mean: 0.488; std: 0.013

    similarity encoding
    Accuracy score:  mean: 0.658; std: 0.017


Plot the results
------------------



.. code-block:: python

    import seaborn
    import matplotlib.pyplot as plt
    plt.figure(figsize=(4, 3))
    ax = seaborn.boxplot(data=pd.DataFrame(all_scores), orient='h')
    plt.ylabel('Encoding', size=20)
    plt.xlabel('Prediction accuracy     ', size=20)
    plt.yticks(size=20)
    plt.tight_layout()




.. image:: /auto_examples/images/sphx_glr_03_midwest_survey_001.png
    :class: sphx-glr-single-img




We can see that encoding the data using a SimilarityEncoder instead of
OneHotEncoder helps a lot in improving the cross validation score!


**Total running time of the script:** ( 0 minutes  6.919 seconds)


.. _sphx_glr_download_auto_examples_03_midwest_survey.py:


.. only :: html

 .. container:: sphx-glr-footer
    :class: sphx-glr-footer-example



  .. container:: sphx-glr-download

     :download:`Download Python source code: 03_midwest_survey.py <03_midwest_survey.py>`



  .. container:: sphx-glr-download

     :download:`Download Jupyter notebook: 03_midwest_survey.ipynb <03_midwest_survey.ipynb>`


.. only:: html

 .. rst-class:: sphx-glr-signature

    `Gallery generated by Sphinx-Gallery <https://sphinx-gallery.readthedocs.io>`_
