{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nPredicting the salary of employees\n==================================\n\nThe `employee salaries <https://catalog.data.gov/dataset/employee-salaries-2016>`_\ndataset contains information\nabout annual salaries (year 2016) for more than 9,000 employees of the \nMontgomery County (Maryland, US). In this example, we are interested\nin predicting the column *Current Annual Salary*\ndepending on a mix of clean columns and a dirty column.\nWe choose to benchmark different categorical encodings for\nthe dirty column *Employee Position Title*, that contains\ndirty categorical data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Importing and preprocessing\n--------------------------------\n\nWe first download the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_employee_salaries\nemployee_salaries = fetch_employee_salaries()\nprint(employee_salaries['DESCR'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\ndf = employee_salaries['data']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now, let's carry out some basic preprocessing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "df['Date First Hired'] = pd.to_datetime(df['date_first_hired'])\ndf['Year First Hired'] = df['Date First Hired'].apply(lambda x: x.year)\n# drop rows with NaN in gender\ndf.dropna(subset=['gender'], inplace=True)\n\ntarget_column = 'Current Annual Salary'\ny = df[target_column].values.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Choosing columns\n-----------------\nFor categorical columns that are supposed to be clean, it is \"safe\" to\nuse one hot encoding to transform them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "clean_columns = {\n    'gender': 'one-hot',\n    'department_name': 'one-hot',\n    'assignment_category': 'one-hot',\n    'Year First Hired': 'numerical'}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We then choose the categorical encoding methods we want to benchmark\nand the dirty categorical variable:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "encoding_methods = ['one-hot', 'target', 'similarity', 'minhash']\ndirty_column = 'employee_position_title'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Creating a learning pipeline\n----------------------------\nThe encoders for both clean and dirty data are first imported:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom dirty_cat import SimilarityEncoder, TargetEncoder, MinHashEncoder\n\nencoders_dict = {\n    'one-hot': OneHotEncoder(handle_unknown='ignore', sparse=False),\n    'similarity': SimilarityEncoder(similarity='ngram'),\n    'target': TargetEncoder(handle_unknown='ignore'),\n    'minhash': MinHashEncoder(n_components=100),\n    'numerical': FunctionTransformer(None)}\n\n# We then create a function that takes one key of our ``encoders_dict``,\n# returns a pipeline object with the associated encoder,\n# as well as a Scaler and a RidgeCV regressor:\n\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\ndef make_pipeline(encoding_method):\n    # static transformers from the other columns\n    transformers = [(enc + '_' + col, encoders_dict[enc], [col])\n                    for col, enc in clean_columns.items()]\n    # adding the encoded column\n    transformers += [(encoding_method, encoders_dict[encoding_method],\n                      [dirty_column])]\n    pipeline = Pipeline([\n        # Use ColumnTransformer to combine the features\n        ('union', ColumnTransformer(\n            transformers=transformers,\n            remainder='drop')),\n        ('scaler', StandardScaler(with_mean=False)),\n        ('clf', RidgeCV())\n    ])\n    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Fitting each encoding methods with a RidgeCV\n--------------------------------------------\nEventually, we loop over the different encoding methods,\ninstantiate each time a new pipeline, fit it\nand store the returned cross-validation score:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import RidgeCV\nfrom sklearn.model_selection import KFold, cross_val_score\nimport numpy as np\n\nall_scores = dict()\n\ncv = KFold(n_splits=5, random_state=12, shuffle=True)\nscoring = 'r2'\nfor method in encoding_methods:\n    pipeline = make_pipeline(method)\n    scores = cross_val_score(pipeline, df, y, cv=cv, scoring=scoring)\n    print('{} encoding'.format(method))\n    print('{} score:  mean: {:.3f}; std: {:.3f}\\n'.format(\n        scoring, np.mean(scores), np.std(scores)))\n    all_scores[method] = scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the results\n--------------------\nFinally, we plot the scores on a boxplot:\nWe notice that the MinHashEncoder does not performs as well compared to \nother encoding methods.\nThere are two reasons for that: the MinHashEncoder performs better\nwith tree-based models than linear models (\n`see example 03<sphx_glr_auto_examples_03_fit_predict_plot_midwest_survey.py>`)\n, and also\nincreasing `n_components` improves performances. `n_components` around 300 \ntend to lead to good prediction performance, but with more computational\ncost.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(4, 3))\nax = seaborn.boxplot(data=pd.DataFrame(all_scores), orient='h')\nplt.ylabel('Encoding', size=20)\nplt.xlabel('Prediction accuracy     ', size=20)\nplt.yticks(size=20)\nplt.tight_layout()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}