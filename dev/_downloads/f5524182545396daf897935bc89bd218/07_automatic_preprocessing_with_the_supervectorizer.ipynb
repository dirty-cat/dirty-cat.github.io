{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Automatic pre-processing with the SuperVectorizer\n\nIn this notebook, we introduce the `SuperVectorizer`, which automatically\napplies transformers to the different columns of a dataset.\nWe demonstrate that on the `employee salaries` dataset.\n\n\n.. |OneHotEncoder| replace::\n    :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |ColumnTransformer| replace::\n    :class:`~sklearn.compose.ColumnTransformer`\n\n.. |RandomForestRegressor| replace::\n    :class:`~sklearn.ensemble.RandomForestRegressor`\n\n.. |SE| replace:: :class:`~dirty_cat.SimilarityEncoder`\n\n.. |permutation importances| replace::\n    :func:`~sklearn.inspection.permutation_importance`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing the data\nLet's fetch the dataset, and load X and y:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom dirty_cat.datasets import fetch_employee_salaries\n\npd.set_option('display.max_rows', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.width', None)\npd.set_option('display.max_colwidth', None)\n\nemployee_salaries = fetch_employee_salaries()\nprint(employee_salaries['DESCR'])\n\nX: pd.DataFrame = employee_salaries['data']\ny = employee_salaries['target']\n# We'll drop a few columns we don't want\nX.drop(\n    [\n        'Current Annual Salary',  # Too linked with target\n        'full_name',  # Not relevant to the analysis\n        '2016_gross_pay_received',  # Too linked with target\n        '2016_overtime_pay',  # Too linked with target\n        'date_first_hired'  # Redundant with \"year_first_hired\"\n    ],\n    axis=1,\n    inplace=True\n)\n\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, random_state=42\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the vectorizer\nHere is a simple workflow with the `SuperVectorizer` using a `Pipeline`:\n\nIt's the typical and recommended way of using it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\n# For scikit-learn 0.24, we need to require the experimental feature\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nfrom sklearn.pipeline import Pipeline\n\nfrom dirty_cat import SuperVectorizer\n\n\npipeline = Pipeline([\n    ('vectorizer', SuperVectorizer(auto_cast=True)),\n    ('clf', HistGradientBoostingRegressor(random_state=42))\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "For reference, let's perform a cross-validation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n\n\nscores = cross_val_score(pipeline, X, y, scoring='r2')\n\nprint(f'{scores=}')\nprint(f'mean={np.mean(scores)}')\nprint(f'std={np.std(scores)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Compared to\n`example 02<sphx_glr_auto_examples_02_fit_predict_plot_employee_salaries.py>`,\nthe mean score is a pretty much the same as the all-|SE|.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing what it does\nLet's perform the same workflow, but without the `Pipeline`, so we can\nanalyze its mechanisms along the way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sup_vec = SuperVectorizer(auto_cast=True)\n\nX_train_enc = sup_vec.fit_transform(X_train, y_train)\nX_test_enc = sup_vec.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inspecting the features created\nOnce it has been trained on data,\nwe can print the transformers and the columns assignment it creates:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(sup_vec.transformers_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what is being passed to the |ColumnTransformer| under the hood.\nIf you're familiar with how the later works, it should be very intuitive.\nWe can notice it classified the columns \"gender\" and \"assignment_category\"\nas low cardinality string variables.\nA |OneHotEncoder| will be applied to these columns.\n\nThe vectorizer actually makes the difference between string variables\n(data type ``object`` and ``string``) and categorical variables\n(data type ``category``).\n\nNext, we can have a look at the encoded feature names.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Before:\nprint(X.columns.to_list())\n# After :\nfeature_names = sup_vec.get_feature_names()\nprint(feature_names[:8])\nprint(len(feature_names))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, it created a new column for each unique value.\nThis is because we used |SE| on the column \"division\",\nwhich was classified as a high cardinality string variable.\n(default values, see `SuperVectorizer`'s docstring).\nIn total, we have 1212 encoded columns.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature importance in the statistical model\nIn this section, we will train a regressor, and plot the feature importances\n.. topic:: Note:\n\n   we will plot the feature importances computed by the\n   |RandomForestRegressor|, but you should use |permutation importances|\n   instead (which are much more accurate)\n\n   We chose the former over the later for the sake of performance.\n\nFirst, let's train the |RandomForestRegressor|,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\n\n\nregressor = RandomForestRegressor(n_estimators=25, random_state=42)\nregressor.fit(X_train_enc, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And then plot the feature importances.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\n\n# Getting feature importances\nimportances = regressor.feature_importances_\nstd = np.std(\n    [\n        tree.feature_importances_\n        for tree in regressor.estimators_\n    ],\n    axis=0\n)\nindices = np.argsort(importances)[::-1]\n\n# Plotting the results:\n\nplt.figure(figsize=(12, 9))\nplt.title(\"Feature importances\")\nn = 20\nn_indices = indices[:n]\nlabels = np.array(feature_names)[n_indices]\nplt.barh(range(n), importances[n_indices], color=\"b\", yerr=std[n_indices])\nplt.yticks(range(n), labels, size=15)\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can deduce a few things from this data:\nthe three factors that define the most the salary are: being a manager,\nbeing hired for a long time, and have a permanent, full-time job.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}