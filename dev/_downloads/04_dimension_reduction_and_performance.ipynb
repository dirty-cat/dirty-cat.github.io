{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nScalability considerations for similarity encoding\n===================================================\n\nHere we discuss how to apply efficiently SimilarityEncoder to larger\ndatasets: reducing the number of reference categories to \"prototypes\",\neither chosen as the most frequent categories, or with kmeans clustering.\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# Avoid the warning in scikit-learn's LogisticRegression for the change\n# in the solver\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A tool to report memory usage and run time\n-------------------------------------------\n\nFor this example, we build a small tool that reports memory\nusage and compute time of a function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\nimport functools\nimport memory_profiler\n\n\ndef resource_used(func):\n    \"\"\" Decorator that return a function that prints its usage\n    \"\"\"\n\n    @functools.wraps(func)\n    def wrapped_func(*args, **kwargs):\n        t0 = time()\n        mem, out = memory_profiler.memory_usage((func, args, kwargs),\n                                                max_usage=True,\n                                                retval=True)\n        print(\"Run time: %.1is    Memory used: %iMb\"\n              % (time() - t0, mem[0]))\n        return out\n\n    return wrapped_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Importing and preprocessing\n--------------------------------\n\nWe first download the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_traffic_violations\n\ndata = fetch_traffic_violations()\nprint(data['description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndf = pd.read_csv(data['path'])\n\n# Limit to 50 000 rows, for a faster example\ndf = df[:50000].copy()\ndf = df.dropna(axis=0)\ndf = df.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use SimilarityEncoder on the 'description' column. One\ndifficulty is that it many different entries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df['Description'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df['Description'].value_counts()[:20])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we will see,SimilarityEncoder takes a while on such data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SimilarityEncoder with default options\n--------------------------------------\n\nLet us build our vectorizer, using a ColumnTransformer to combine\none-hot encoding and similarity encoding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom dirty_cat import SimilarityEncoder\n\nsim_enc = SimilarityEncoder(similarity='ngram')\n\ny = df['Violation Type']\n\n# clean columns\ntransformers = [('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'),\n                 ['Alcohol',\n                  'Arrest Type',\n                  'Belts',\n                  'Commercial License',\n                  'Commercial Vehicle',\n                  'Fatal',\n                  'Gender',\n                  'HAZMAT',\n                  'Property Damage',\n                  'Race',\n                  'Work Zone']),\n                ('pass', 'passthrough', ['Year']),\n                ]\n\ncolumn_trans = ColumnTransformer(\n    # adding the dirty column\n    transformers=transformers + [('sim_enc', sim_enc, ['Description'])],\n    remainder='drop')\n\nt0 = time()\nX = column_trans.fit_transform(df)\nt1 = time()\nprint('Time to vectorize: %s' % (t1 - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run a cross-validation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model, pipeline, model_selection\n\nlog_reg = linear_model.LogisticRegression()\n\nmodel = pipeline.make_pipeline(column_trans, log_reg)\nresults = resource_used(model_selection.cross_validate)(model, df, y, )\nprint(\"Cross-validation score: %s\" % results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store results for later\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores = dict()\nscores['Default options'] = results['test_score']\ntimes = dict()\ntimes['Default options'] = results['fit_time']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Most frequent strategy to define prototypes\n---------------------------------------------\n\nThe most frequent strategy selects the n most frequent values in a dirty\ncategorical variable to reduce the dimensionality of the problem and thus\nspeed things up. We select manually the number of prototypes we want to use.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_enc = SimilarityEncoder(similarity='ngram', categories='most_frequent',\n                            n_prototypes=100)\n\ncolumn_trans = ColumnTransformer(\n    # adding the dirty column\n    transformers=transformers + [('sim_enc', sim_enc, ['Description'])],\n    remainder='drop')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check now that prediction is still as good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = pipeline.make_pipeline(column_trans, log_reg)\nresults = resource_used(model_selection.cross_validate)(model, df, y)\nprint(\"Cross-validation score: %s\" % results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store results for later\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores['Most frequent'] = results['test_score']\ntimes['Most frequent'] = results['fit_time']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "KMeans strategy to define prototypes\n---------------------------------------\n\nK-means strategy is also a dimensionality reduction technique.\nSimilarityEncoder can apply a K-means and nearest neighbors algorithm\nto find the prototypes. The number of prototypes is set manually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_enc = SimilarityEncoder(similarity='ngram', categories='k-means',\n                            n_prototypes=100)\n\ncolumn_trans = ColumnTransformer(\n    # adding the dirty column\n    transformers=transformers + [('sim_enc', sim_enc, ['Description'])],\n    remainder='drop')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check now that prediction is still as good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = pipeline.make_pipeline(column_trans, log_reg)\nresults = resource_used(model_selection.cross_validate)(model, df, y)\nprint(\"Cross-validation score: %s\" % results['test_score'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Store results for later\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "scores['KMeans'] = results['test_score']\ntimes['KMeans'] = results['fit_time']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plot a summary figure\n----------------------\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn\nimport matplotlib.pyplot as plt\n\n_, (ax1, ax2) = plt.subplots(nrows=2, figsize=(4, 3))\nseaborn.boxplot(data=pd.DataFrame(scores), orient='h', ax=ax1)\nax1.set_xlabel('Prediction accuracy', size=16)\n[t.set(size=16) for t in ax1.get_yticklabels()]\n\nseaborn.boxplot(data=pd.DataFrame(times), orient='h', ax=ax2)\nax2.set_xlabel('Computation time', size=16)\n[t.set(size=16) for t in ax2.get_yticklabels()]\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Reduce memory usage during encoding using float32\n----------------------------------------------------------------\n\nWe use a float32 dtype in this example to show some speed and memory gains.\nThe use of the scikit-learn model may upcast to float64 (depending on the used\nalgorithm). The memory savings will then happen during the encoding.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nsim_enc = SimilarityEncoder(similarity='ngram', dtype=np.float32,\n                            categories='most_frequent', n_prototypes=100)\n\ny = df['Violation Type']\n# cast the year column to float32\ndf['Year'] = df['Year'].astype(np.float32)\n# clean columns\ntransformers = [('one_hot', OneHotEncoder(sparse=False, dtype=np.float32,\n                                          handle_unknown='ignore'),\n                 ['Alcohol',\n                  'Arrest Type',\n                  'Belts',\n                  'Commercial License',\n                  'Commercial Vehicle',\n                  'Fatal',\n                  'Gender',\n                  'HAZMAT',\n                  'Property Damage',\n                  'Race',\n                  'Work Zone']),\n                ('pass', 'passthrough', ['Year']),\n                ]\n\ncolumn_trans = ColumnTransformer(\n    # adding the dirty column\n    transformers=transformers + [('sim_enc', sim_enc, ['Description'])],\n    remainder='drop')\n\nt0 = time()\nX = column_trans.fit_transform(df)\nt1 = time()\nprint('Time to vectorize: %s' % (t1 - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run a cross-validation to confirm the memory footprint reduction\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = pipeline.make_pipeline(column_trans, log_reg)\nresults = resource_used(model_selection.cross_validate)(model, df, y, )\nprint(\"Cross-validation score: %s\" % results['test_score'])"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}