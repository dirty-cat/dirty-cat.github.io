{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nBasic dirty_cat example: manipulating and looking at data\n=========================================================\n\nlet's try to understand how embedding dirty categorical variables with\n3gram similarity can help in learning better models\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "What do we mean by dirty data?\n-------------------------------------------------\n\nLet's look at a dataset called employee salaries:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\nfrom dirty_cat import datasets\n\nemployee_salaries = datasets.fetch_employee_salaries()\ndf = pd.read_csv(employee_salaries['path'])\nprint(df.head(n=5))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here is how the columns are distributed:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df.nunique())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some numerical columns (Gross pay, etc..) and some obvious categorical\ncolumns such as full_name\nof course have many different values. but it is also the case\nfor other categorical columns  such as Employee position title:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sorted_values = df['Employee Position Title'].sort_values().unique()\nfor i in range(5):\n    print(sorted_values[i] + '\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Here we go! See how there are 3 kinds of Accountant/Auditor? I,II,and III.\nNow, there are some reason why traditional word-encoding methods won't work\nvery well.\n\n* Using simple one-hot encoding will create orthogonal features,\n  whereas it is clear that those 3 terms have a lot in common.\n\n* If we wanted to use word embedding methods such as word2vec,\n  we would have to go through a cleaning phase: those algorithms\n  are not trained to work on data such as 'Accountant/Auditor I'.\n  However, that can be unsafe and take a long time.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Encoding categorical data using SimilarityEncoder\n-------------------------------------------------\nThat's where our encoders get into play. In order to robustly\nembed dirty semantic data, the SimilarityEncoder creates a similarity\nmatrix based on the 3-gram structure of the data.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat import SimilarityEncoder\n\nsimilarity_encoder = SimilarityEncoder(similarity='ngram')\ntransformed_values = similarity_encoder.fit_transform(\n    sorted_values.reshape(-1, 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------\nPlotting the new feature map using multi-dimensional scaling\n------------------------------------------------------------\nlets now plot a couple points at random using a low-dimensional representation\nto get an intuition of what the similarity encoder is doing:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.manifold import MDS\n\nmds = MDS(dissimilarity='precomputed', n_init=10, random_state=42)\ntwo_dim_data = mds.fit_transform(\n    1 - transformed_values)  # transformed values lie\n# in the 0-1 range, so 1-transformed_value yields a positive dissimilarity matrix\nprint(two_dim_data.shape)\nprint(sorted_values.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "we first quickly fit a KNN so that the plots does not get too busy:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy as np\n\nn_points = 5\nnp.random.seed(42)\nfrom sklearn.neighbors import NearestNeighbors\n\nrandom_points = np.random.choice(len(similarity_encoder.categories_[0]),\n                                 n_points, replace=False)\nnn = NearestNeighbors(n_neighbors=2).fit(transformed_values)\n_, indices_ = nn.kneighbors(transformed_values[random_points])\nindices = np.unique(indices_.squeeze())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and then plot it, adding the categories in the scatter plot:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n\nf, ax = plt.subplots()\nax.scatter(x=two_dim_data[indices, 0], y=two_dim_data[indices, 1])\n# adding the legend\nfor x in indices:\n    ax.text(x=two_dim_data[x, 0], y=two_dim_data[x, 1], s=sorted_values[x],\n            fontsize=8)\nax.set_title(\n    'multi-dimensional-scaling representation using a 3gram similarity matrix')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "------------------------------------------------------------\nHeatmap of the similarity matrix\n------------------------------------------------------------\nWe can also plot the distance matrix for those observations:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "f2, ax2 = plt.subplots(figsize=(6, 6))\ncax2 = ax2.matshow(transformed_values[indices, :][:, indices])\nax2.set_yticks(np.arange(len(indices)))\nax2.set_xticks(np.arange(len(indices)))\nax2.set_yticklabels(sorted_values[indices], rotation='30')\nax2.set_xticklabels(sorted_values[indices], rotation='60', ha='right')\nax2.xaxis.tick_bottom()\nf2.colorbar(cax2)\nf2.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As shown in the previous plot, we see that \"communication Equipment technician\"'s\nnearest neighbor is \"telecommunication technician\", although it is also\nvery close to senior \"supply technician\": therefore, we grasp the\n\"communication\" part (not initially present in the category as a unique word)\nas well as the technician part of this category.\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}