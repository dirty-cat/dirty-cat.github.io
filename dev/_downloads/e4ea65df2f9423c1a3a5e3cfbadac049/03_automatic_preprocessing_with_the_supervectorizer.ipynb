{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Automatic pre-processing with the SuperVectorizer\n\nIn this notebook, we introduce the |SV|, which automatically\nturns a heterogeneous dataset into a numerical representation, finding\nthe right transformers to apply to the different columns.\n\nWe demonstrate it on the `employee salaries` dataset.\n\n.. |SV| replace::\n    :class:`~dirty_cat.SuperVectorizer`\n\n.. |OneHotEncoder| replace::\n    :class:`~sklearn.preprocessing.OneHotEncoder`\n\n.. |ColumnTransformer| replace::\n    :class:`~sklearn.compose.ColumnTransformer`\n\n.. |RandomForestRegressor| replace::\n    :class:`~sklearn.ensemble.RandomForestRegressor`\n\n.. |SE| replace:: :class:`~dirty_cat.SimilarityEncoder`\n\n.. |permutation importances| replace::\n    :func:`~sklearn.inspection.permutation_importance`\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Importing the data\nLet's fetch the dataset, and load X (the employees' features) and y\n(the salary to predict):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_employee_salaries\nemployee_salaries = fetch_employee_salaries()\nprint(employee_salaries['DESCR'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X = employee_salaries['data']\ny = employee_salaries['target']\n# We'll drop a few columns we don't want\nX.drop(\n    [\n        'Current Annual Salary',  # Too linked with target\n        'full_name',  # Not relevant to the analysis\n        '2016_gross_pay_received',  # Too linked with target\n        '2016_overtime_pay',  # Too linked with target\n        'date_first_hired'  # Redundant with \"year_first_hired\"\n    ],\n    axis=1,\n    inplace=True\n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The data are in a fairly complex and heterogeneous dataframe:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The challenge is to turn this dataframe into a form suited for\nmachine learning.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Using the SuperVectorizer in a supervised-learning pipeline\n\nAssembling the |SV| in a pipeline with a powerful learner,\nsuch as gradient boosted trees, gives **a machine-learning method that\ncan be readily applied to the dataframe**.\n\nIt's the typical and recommended way of using it.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "# For scikit-learn 0.24, we need to require the experimental feature\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingRegressor\n\nfrom sklearn.pipeline import Pipeline\n\nfrom dirty_cat import SuperVectorizer\n\npipeline = Pipeline([\n    ('vectorizer', SuperVectorizer(auto_cast=True)),\n    ('clf', HistGradientBoostingRegressor(random_state=42))\n])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's perform a cross-validation to see how well this model predicts\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(pipeline, X, y, scoring='r2')\n\nimport numpy as np\nprint(f'{scores=}')\nprint(f'mean={np.mean(scores)}')\nprint(f'std={np.std(scores)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The prediction perform here is pretty much as good as in `example\n02<sphx_glr_auto_examples_02_fit_predict_plot_employee_salaries.py>`,\nbut the code here is much simpler as it does not involve specifying\ncolumns manually.\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Analyzing the features created\n\nLet us perform the same workflow, but without the `Pipeline`, so we can\nanalyze its mechanisms along the way.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sup_vec = SuperVectorizer(auto_cast=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We split the data between train and test, and transform them:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.15, random_state=42\n)\n\nX_train_enc = sup_vec.fit_transform(X_train, y_train)\nX_test_enc = sup_vec.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inspecting the features created\nOnce it has been trained on data,\nwe can print the transformers and the columns assignment it creates:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sup_vec.transformers_"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "This is what is being passed to the |ColumnTransformer| under the hood.\nIf you're familiar with how the later works, it should be very intuitive.\nWe can notice it classified the columns \"gender\" and \"assignment_category\"\nas low cardinality string variables.\nA |OneHotEncoder| will be applied to these columns.\n\nThe vectorizer actually makes the difference between string variables\n(data type ``object`` and ``string``) and categorical variables\n(data type ``category``).\n\nNext, we can have a look at the encoded feature names.\n\nBefore encoding:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "X.columns.to_list()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "After encoding (we only plot the first 8 feature names):\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "feature_names = sup_vec.get_feature_names()\nfeature_names[:8]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we can see, it created a new column for each unique value.\nThis is because we used |SE| on the column \"division\",\nwhich was classified as a high cardinality string variable.\n(default values, see |SV|'s docstring).\n\nIn total, we have 1212 encoded columns.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "len(feature_names)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Feature importance in the statistical model\nIn this section, we will train a regressor, and plot the feature importances\n.. topic:: Note:\n\n   To minimize compute time, use the feature importances computed by the\n   |RandomForestRegressor|, but you should prefer |permutation importances|\n   instead (which are less subject to biases)\n\nFirst, let's train the |RandomForestRegressor|,\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestRegressor\nregressor = RandomForestRegressor()\nregressor.fit(X_train_enc, y_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Getting the feature importances\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "importances = regressor.feature_importances_\nstd = np.std(\n    [\n        tree.feature_importances_\n        for tree in regressor.estimators_\n    ],\n    axis=0\n)\nindices = np.argsort(importances)[::-1]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Plotting the results:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\nplt.figure(figsize=(12, 9))\nplt.title(\"Feature importances\")\nn = 20\nn_indices = indices[:n]\nlabels = np.array(feature_names)[n_indices]\nplt.barh(range(n), importances[n_indices], color=\"b\", yerr=std[n_indices])\nplt.yticks(range(n), labels, size=15)\nplt.tight_layout(pad=1)\nplt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can deduce from this data that the three factors that define the\nmost the salary are: being a manager, being hired for a long time, and\nhave a permanent, full-time job :).\n\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}