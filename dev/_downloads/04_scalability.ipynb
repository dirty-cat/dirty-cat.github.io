{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\nScalability considerations for  similarity encoding\n===================================================\n\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "A tool to report memory usage and run time of a function\n---------------------------------------------------------\n\nFor the sake of this example, we build a small tool that reports memory\nusage and compute time of a function\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from time import time\nimport functools\nimport memory_profiler\n\n\n\ndef resource_used(func):\n    \"\"\" Decorator that return a function that prints its usage\n    \"\"\"\n    @functools.wraps(func)\n    def wrapped_func(*args, **kwargs):\n        t0 = time()\n        mem, out = memory_profiler.memory_usage((func, args, kwargs),\n                                                max_usage=True,\n                                                retval=True)\n        print(\"Run time: %.1is    Memory used: %iMb\"\n              % (time() - t0, mem[0]))\n        return out\n    return wrapped_func"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Data Importing and preprocessing\n--------------------------------\n\nWe first download the dataset:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_traffic_violations\n\ndata = fetch_traffic_violations()\nprint(data['description'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then we load it:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n\ndf = pd.read_csv(data['path'])\n\n# Limit to 50 000 rows, for a faster example\ndf = df[:50000].copy()\ndf = df.dropna(axis=0)\ndf = df.reset_index()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will use SimilarityEncoder on the 'description' column. One\ndifficulty is that it many different entries\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df['Description'].nunique())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "print(df['Description'].value_counts()[:30])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "As we will see,SimilarityEncoder takes a while on such data\n\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SimilarityEncoder with default options\n--------------------------------------\n\nLet us build our vectorizer, using a ColumnTransformer to combine\none-hot encoding and similarity encoding\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom dirty_cat import SimilarityEncoder\n\nsim_enc = SimilarityEncoder(similarity='ngram', handle_unknown='ignore')\n\ny = df['Violation Type']\n\n# clean columns\ntransformers = [('one_hot', OneHotEncoder(sparse=False, handle_unknown='ignore'),\n                 ['Alcohol',\n                  'Arrest Type',\n                  'Belts',\n                  'Commercial License',\n                  'Commercial Vehicle',\n                  'Fatal',\n                  'Gender',\n                  'HAZMAT',\n                  'Property Damage',\n                  'Race',\n                  'Work Zone']),\n                ('pass', 'passthrough', ['Year']),\n                ]\n\ncolumn_trans = ColumnTransformer(\n    # adding the dirty column\n    transformers=transformers + [('sim_enc', sim_enc, ['Description'])],\n    remainder='drop')\n\n\nt0 = time()\nX = column_trans.fit_transform(df)\nt1 = time()\nprint('Time to vectorize: %s' % (t1 - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can run a cross-validation\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn import linear_model, pipeline, model_selection\nlog_ref = linear_model.LogisticRegression()\n\nmodel = pipeline.make_pipeline(column_trans, log_ref)\nprint(\"Cross-validation score: %s\" %\n      resource_used(model_selection.cross_val_score)(model, df, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SimilarityEncoder with most frequent strategy\n---------------------------------------------\n\nThe most frequent strategy selects the n most frequent values in a dirty\ncategorical variable to reduce the dimensionality of the problem and thus\nspeed things up. We select manually the number of prototypes we want to use.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_enc = SimilarityEncoder(similarity='ngram', categories='most_frequent', n_prototypes=100)\n\ncolumn_trans = ColumnTransformer(\n    # adding the dirty column\n    transformers=transformers + [('sim_enc', sim_enc, ['Description'])],\n    remainder='drop')\n\nt0 = time()\nX = column_trans.fit_transform(df)\nt1 = time()\nprint('Time to vectorize: %s' % (t1 - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check now that prediction is still as good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = pipeline.make_pipeline(column_trans, log_ref)\nprint(\"Cross-validation score: %s\" %\n      resource_used(model_selection.cross_val_score)(model, df, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "SimilarityEncoder with k-means strategy\n---------------------------------------\n\nK-means strategy is also a dimensionality reduction technique. But we apply\na K-means and nearest neighbors algorithm to find the prototypes. The number\nof prototypes is set manually.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "sim_enc = SimilarityEncoder(similarity='ngram', categories='k-means', n_prototypes=100)\n\ncolumn_trans = ColumnTransformer(\n    # adding the dirty column\n    transformers=transformers + [('sim_enc', sim_enc, ['Description'])],\n    remainder='drop')\n\nt0 = time()\nX = column_trans.fit_transform(df)\nt1 = time()\nprint('Time to vectorize: %s' % (t1 - t0))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Check now that prediction is still as good\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "model = pipeline.make_pipeline(column_trans, log_ref)\nprint(\"cross-validation score: %s\" %\n      resource_used(model_selection.cross_val_score)(model, df, y))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}