{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n# Semantic variation in the \"Midwest\"\nHere's some survey data with one dirty column, consisting of\nan open-ended question, on which one-hot encoding does not work well.\nThe other columns are more traditional categorical or numerical\nvariables.\n\nLet's see how different encoding for the dirty column impact on the\nscore of a classification problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Loading the data\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from dirty_cat.datasets import fetch_midwest_survey\nimport pandas as pd\n\ndataset = fetch_midwest_survey()\ndf = pd.read_csv(dataset['path']).astype(str)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The challenge with this data is that it contains a free-form input\ncolumn, where people put whatever they want:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "dirty_column = 'In your own words, what would you call the part of the country you live in now?'\nprint(df[dirty_column].value_counts()[-10:])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Separating clean, and dirty columns as well a a column we will try to predict\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "target_column = 'Location (Census Region)'\nclean_columns = [\n    'Personally identification as a Midwesterner?',\n    'Illinois in MW?',\n    'Indiana in MW?',\n    'Kansas in MW?',\n    'Iowa in MW?',\n    'Michigan in MW?',\n    'Minnesota in MW?',\n    'Missouri in MW?',\n    'Nebraska in MW?',\n    'North Dakota in MW?',\n    'Ohio in MW?',\n    'South Dakota in MW?',\n    'Wisconsin in MW?',\n    'Arkansas in MW?',\n    'Colorado in MW?',\n    'Kentucky in MW?',\n    'Oklahoma in MW?',\n    'Pennsylvania in MW?',\n    'West Virginia in MW?',\n    'Montana in MW?',\n    'Wyoming in MW?',\n    'Gender',\n    'Age',\n    'Household Income',\n    'Education']\ny = df[target_column].values.ravel()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## A pipeline for data fitting and prediction\nWe first import the right encoders to transform our clean/dirty data:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import FunctionTransformer, OneHotEncoder\nfrom dirty_cat import SimilarityEncoder, MinHashEncoder,\\\n    GapEncoder\n\nencoder_dict = {\n    'one-hot': OneHotEncoder(handle_unknown='ignore', sparse=False),\n    'similarity': SimilarityEncoder(similarity='ngram'),\n    'minhash': MinHashEncoder(),\n    'gap': GapEncoder(),\n    'num': FunctionTransformer(None)\n}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "All the clean columns are encoded once and for all, but since we\nbenchmark different categorical encodings for the dirty variable,\nwe create a function that takes an encoding as an input, and returns a \\\nscikit-learn pipeline for our problem.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import RandomForestClassifier\n\n\ndef make_pipeline(encoding_method):\n    # static transformers from the other columns\n    transformers = [('one-hot-clean', encoder_dict['one-hot'], clean_columns)]\n    # adding the encoded column\n    transformers += [(encoding_method + '-dirty', encoder_dict[encoding_method],\n                      [dirty_column])]\n    pipeline = Pipeline([\n        # Use ColumnTransformer to combine the features\n        ('union', ColumnTransformer(\n            transformers=transformers,\n            remainder='drop')),\n        ('scaler', StandardScaler(with_mean=False)),\n        ('classifier', RandomForestClassifier(random_state=5))\n    ])\n\n    return pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Evaluation of different encoding methods\nWe then loop over encoding methods, scoring the different pipeline predictions\nusing a cross validation score:\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\n\ncv = StratifiedKFold(n_splits=3, random_state=12, shuffle=True)\nall_scores = {}\nfor method in ['one-hot', 'similarity', 'minhash', 'gap']:\n    pipeline = make_pipeline(method)\n    # Now predict the census region of each participant\n    scores = cross_val_score(pipeline, df, y, cv=cv)\n    all_scores[method] = scores\n\n    print('%s encoding' % method)\n    print('Accuracy score:  mean: %.3f; std: %.3f\\n'\n          % (scores.mean(), scores.std()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Plot the results\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import seaborn\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(4, 3))\nax = seaborn.boxplot(data=pd.DataFrame(all_scores), orient='h')\nplt.ylabel('Encoding', size=20)\nplt.xlabel('Prediction accuracy     ', size=20)\nplt.yticks(size=20)\nplt.tight_layout()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We can see that encoding the data using a SimilarityEncoder or MinhashEncoder\ninstead of OneHotEncoder helps a lot in improving the cross validation score!\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}